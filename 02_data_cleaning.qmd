---
title: "Data Cleaning --- Behavioural addictions research past & present: A bibliographic review"
author: "Rob Heirene$^1$"
date: "`r format(Sys.time(), '%d %B, %Y')`"
execute: # prevent code and execution messages showing
  echo: false
  warning: false
  message: false
format: html
---

```{r output=FALSE, warning = FALSE, messages=FALSE}
# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install

# Load in packages using `groundhog` to ensure consistency of the versions used here:

library(groundhog) # Load
 
# set.groundhog.folder("C:/Users/rhei4496/Groundhog packages") # Set in a writable directory
get.groundhog.folder()

# List desired packages:
packages <- c("tidyverse", 
              'readr', # Load dataset from GitHib
              'RCurl', # Load dataset from GitHib
              'gtExtras', # Add colours to gt tables
              'transformr', # Needed for certain animations (dumbell lines)
              'png',# Helps render gganimate plots
              'gifski', # Helps render gganimate plots
              'rmarkdown', # Helps render gganimate plots
              'av', # render gganimate plots as videos
              'Cairo', # Anti-aliasing for the line plots (smoothing output)
              'ggtext', # make fancy labels in plots
              'sysfonts', # Special fonts for figures
              'showtext', # Special fonts for figures
              'scico', # Colour palette
               'maps', # Get map/geographic data for author locations
              'purrr', # Help unnest city and author names across papers  equally
              'kableExtra', # Make tabless
              'formattable', #  Add visualisations to tables
              'gt', # Alternative table options
              'gtsummary', # Create summary tables
              'scales', # Allows for the removal of scientific notation in axis labels
              'ggrain', # Make rain cloud plots
              'waffle', # make waffle plots for proportions
              'networkD3', # Make Sankey plots to show relationships
              'patchwork', # Join plots in multipanel layouts
              'pwr', # Check statistical power
              'car', # Perform ANCOVA stats tests
              'rstatix', # Perform ANCOVA stats tests
              'ggpubr', # Plots for linearity checks 
              'broom', # Print summaries of statistical test outputs
              'psych', # get detailed summary figures to Supplement statistical tests
              'ggstatsplot', # Plots with statistical outputs
              'janitor', # Make column names consistent format
              'caret', # Compute model performance indices
              'sessioninfo', # Detailed session info for reproducibility
              "osfr",
              "readxl",
              # "Gmisc", # Produce prisma flow diagram
              # 'grid', # Produce prisma flow diagram
              # "glue", # Produce prisma flow diagram
              "apa", # print test results in apa format
              "apaTables", # print test results in apa format
              "ggh4x", # truncate graph axis lines
              "truncnorm", # Generate normally distributed data with limits
              "ComplexUpset" # produce upset plots
)
# Load desired package with versions specific to project start date:
groundhog.library(packages, "2024-05-30")
                  # force.install=TRUE) 
                  # tolerate.R.version = '4.4.0')

# groundhog.library(packages, "2023-12-29") 

```

```{r message = FALSE, warning = FALSE}

## Setup presentation & graph specifications. Set up a standard theme for plots/data visualisations:

# Load new font for figures/graphs
font_add_google("Poppins")
font_add_google("Reem Kufi", "Reem Kufi")
font_add_google("Share Tech Mono", "techmono")
windowsFonts(`Segoe UI` = windowsFont('Segoe UI'))
showtext_auto()
showtext_auto(enable = TRUE)

# Save new theme for figures/graphs.This will determine the layout, presentation, font type and font size used in all data visualisations presented here:
plot_theme<- theme_classic() +
  theme(
    text=element_text(family="Poppins"),
    plot.title = element_text(hjust = 0.5, size = 16),
          plot.subtitle = element_text(hjust = 0.5, size = 13),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
    plot.caption = element_text(size = 12),
    legend.title=element_text(size=12), 
    legend.text=element_text(size=10)
        ) 
```

# Load Data

```{r results=FALSE, warning=FALSE, message=FALSE}
# Now I'll load in the dataset and do a little cleaning. See comments in the code chunk below for exclusions.

#| code-fold: true
#| code-summary: "Code: load dataset"

url_behav_addic_data_link <- "https://raw.githubusercontent.com/rheirene/pub-med-scape-behav-addictions/main/Data%20extraction/combined_results_clean.csv"

raw_data <- read_csv(url_behav_addic_data_link) %>%
  as_tibble()

str(raw_data)

```

# Filter data

Despite my best efforts with manual searching, my explorations of this dataset revealed that there are a few erratums/corrigendums and one notice of retraction included in the scraped data. Let's remove these before moving forward:

```{r}
filtered_data <- raw_data %>%
  filter(str_detect(Publication_Type, "Erratum") | 
         str_detect(Publication_Type, "corrigendum") | 
         str_detect(Publication_Type, "Retraction")) %>% 
  distinct(PMID, .keep_all = TRUE)

# Take a look at these articles to check we're excluding appropriately:
filtered_data %>%
  filter(str_detect(Publication_Type, "Retraction")) %>%
  # print(n=50)
View()

# Let's now remove these pubs:
data <- raw_data %>% 
  anti_join(filtered_data, by = join_by(PMID))
# View(data)
 
```

# Clean values

Let's take a look at some key variables that we can aggregate the data by and see if the values require any cleaning/processing to make them conssistent and able to be summarised.

### Language

Start by taking a look at the language(s) listed in each article:
```{r}
data %>%
  count(Language) %>%
arrange(desc(n)) %>%
gt()
```

Okay, these aren't too messy and there aren't clear inconsistencies, but there are some duplciate variations (e.g., "emg; spa" and "spa; eng") that we could combine. Let's also fill out the names for ease:

```{r}

language_map <- c(
  "eng" = "English", "ger" = "German", "fre" = "French", "rus" = "Russian", 
  "spa" = "Spanish", "hun" = "Hungarian", "jpn" = "Japanese", "chi" = "Chinese",
  "pol" = "Polish", "ita" = "Italian", "kor" = "Korean", "cze" = "Czech",
  "por" = "Portuguese", "heb" = "Hebrew", "dut" = "Dutch", "gre" = "Greek",
  "swe" = "Swedish", "fin" = "Finnish", "nor" = "Norwegian", "dan" = "Danish",
  "srp" = "Serbian", "tur" = "Turkish", "hrv" = "Croatian", "lit" = "Lithuanian",
  "eng; spa" = "English; Spanish", "spa; eng" = "English; Spanish",
  "por; eng" = "English; Portuguese", "eng; por" = "English; Portuguese",
  "eng; jpn" = "English; Japanese", "jpn; eng" = "English; Japanese",
  "eng; pol" = "English; Polish", "pol; eng" = "English; Polish",
  "eng; tur" = "English; Turkish", "tur; eng" = "English; Turkish",
  "eng; chi" = "English; Chinese",
  "eng; fre" = "English; French",
  "eng; ger" = "English; German",
  "por; spa; eng"	= "Portuguese; Spanish"
)

data_w_languages <- data %>%
  mutate(Language_recoded = language_map[Language]) 

data_w_languages %>%
  distinct(PMID, .keep_all = TRUE) %>%
  count(Language, 
        Language_recoded) %>%
  arrange(desc(n)) %>%
  gt()

```

### Publication type

There are lots of publication types, so let's just see how many there are first:
```{r}
data_w_languages %>%
 distinct(Publication_Type) %>%
  nrow()
```

Okay, that's far too many to display in a table, but the code below  displays them all so that I can visually inspect the different types and develop a categorisation system (I've restricted the table you'll see to the top 20 most common types):
```{r}
data_w_languages %>%
  count(Publication_Type ) %>%
arrange(desc(n)) %>%
  head(20) %>% # Show just top 20 for the doc
gt()
```

Interestingly, retracted articles are labelled as such. How many retracted articles are there and what addiction types do they belong to (note these are retracted articles and not notices of retractions; the only one of these was removed above):

```{r}
data_w_languages %>% #
 filter(str_detect(Publication_Type, "Retracted")) %>%
  distinct(PMID, .keep_all = TRUE) %>%
  group_by(Label) %>%
count() %>%
arrange(desc(n))
```

Okay, some key themes/article types in the data are:

- Comment/Letter/ Editorial
- Clinical trial/RCT/ RCT protocol
- Systematic reviews & meta-analyses
- Case reports

Most of the remaining articles types appeared to refer to just simple "journal article"s and variations of this that aren't worth breaking down.

Also, inspecting the titles alongside the publication types reveals that not all are perfectly accurate., So, when we recode the categories, it would be a good idea to use a combination of publication type and article title to recode whenever appropriate.

Let`s produce a recoded publication type variable with the above categorie plus the general "journal article" category:

```{r}
data_w_languages_pub_types <- data_w_languages %>%
 mutate(Publication_Category = case_when(str_detect(Publication_Type, "Case Reports") ~ "Case Report(s)",
                                         (str_detect(Publication_Type, "Comment") |
                                            str_detect(Publication_Type, "commentary") |
                                            str_detect(Publication_Type, "Editorial") |
                                            str_detect(Publication_Type, "Letter")) ~ "Editorial, Letter, or Comment",
                                          
                                          (str_detect(Publication_Type, "Systematic Review") |
                                            str_detect(Publication_Type, "Meta-Analysis") | 
                                             str_detect(Title, "Systematic Review")| 
                                             str_detect(Title, "Systematic review")| 
                                             str_detect(Title, "systematic review")|
                                             str_detect(Title, "Meta-Analysis")| 
                                             str_detect(Title, "Meta-analysis")| 
                                             str_detect(Title, "meta-analysis")) ~ "Systematic Review/ Meta-Analysis",
                                         (str_detect(Publication_Type, "; Review") |
                                            str_detect(Title, "Review")) ~ "Review",
                                         (str_detect(Publication_Type, "Controlled Trial") |
                                            str_detect(Publication_Type, "controlled trial") |
                                            str_detect(Publication_Type, "Clinical Trial")| 
                                            str_detect(Publication_Type, "Clinical trial")|
                                            str_detect(Publication_Type, "clinical trial")|
                                             str_detect(Title, "Controlled Trial")|
                                            str_detect(Title, "controlled trial")|
                                             str_detect(Title, "Randomised Controlled Trial")| 
                                             str_detect(Title, "randomised controlled trial")|
                                             str_detect(Title, "Randomised Clinical Trial")| 
                                            str_detect(Title, "randomised clinical trial")
                                          ) ~ "Controlled Trial Report or Protocol",
                                         TRUE ~ "Journal Article (non-specified)")) 
  


# For checking strings:
# data_w_languages %>%
#  filter(str_detect(Title, "Clinical Trial")) %>%
#   View()



# Check coding through random selections of articles:
# data_w_languages_pub_types %>%
#   select(Title,
#          Publication_Type,
#          Publication_Category) %>%
#   slice(800:900) %>% # Alter to vary :)
#   # View()
# print(n=100)



# Check coding through specific new category:
# data_w_languages_pub_types %>% 
#   select(Title, 
#          Publication_Type,
#          Publication_Category) %>%
#   filter(Publication_Category == "Controlled Trial Report or Protocol") %>% # Alter to vary :)
#   print(n=100)
```

And now present a summary of new categories:
```{r}
data_w_languages_pub_types %>%
  distinct(PMID, .keep_all = TRUE) %>%
  # group_by(Label) %>%
  count(Publication_Category) %>%
  mutate(Percent = round(n/sum(n)*100,2)) %>%
arrange(desc(n)) %>%
gt()
```


### Location

I want to to visualise this information, but it's going to be quite tricky as each paper has a variable number of authors and therefore institution addresses, all of which are listed in a single (often messy) string within one column in the dataset. I'll have to separate out each author institution and then find a way to extract only the relevant information to be able to geo-locate them.

```{r warning=FALSE, message=FALSE}
#| code-fold: true
#| code-summary: "Code: Extract location data for each paper"

# ***********************The below code is almost all commented out on purpose as the process of extracting and matching city names from the author address column is so computationally taxing that it takes a long time to process.  I've left the code here so that anyone can see how I did it, but I saved the results as a .csv file and now load the data like that***********************

# # as_tibble(data$Author_Address) # Take a look at how the author addresses are structured
# 
# # Okay, so we're going to need to create an ID variable for each paper (this makes the string split and a nest below work bettter than relying on titles), then split the author_address strings into separate addresses, then unnest these into new rows.
# 
# # Whilst I'm splitting and unnesting the author address: I'm going to simultaneously do this for the authors name column.
# 
# 
# # The un-nest doesn't seem to work well when we retain all of the columns in the dataset, so I do it with only id and institution (address) in the data, then join all of the rest of the data set to the unnested rows after this. For this to work, we need to create a dataset that has the ID variable in before splitting the string and and unnesting. Let's do that:
#  data_id <-data %>%
#   rowid_to_column(var = "id")
# 
#  pad_vector <- function(vec, len) {
#   length(vec) <- len
#   return(vec)
#  }
# 
# # Now split the author and address strings and then  unnest it into multiple rows, and finally re-join with the main dataset
# data_locations <- data_id %>%
#     mutate(
#      institution = str_split(Author_Address, ";"), # Split author address column into separate strings For each address
#     author_names = str_split(Full_Author_Name, ";") # Split author name column into separate strings For each Name
#   ) %>%
#   # The below code matches the number of author institutions and author names where discrepancies exist, so the unnest further below works:
#    mutate(
#     max_length = pmax(map_int(institution, length), map_int(author_names, length)),
#     institution = map2(institution, max_length, ~ pad_vector(., .y)),
#     author_names = map2(author_names, max_length, ~ pad_vector(., .y))
#   ) %>%
#   select(id, institution, author_names) %>%
#   unnest(c(institution, author_names)) %>%
#   # Looking at the data at this point, there's a lot of white space around the institutions and author names. Let's remember that now:
#   mutate(institution = str_trim(institution, side = "both"),
#          author_names = str_trim(author_names, side = "both")) %>%
#   full_join(data_id, by = "id") %>%
#   # Whilst we're doing this, we'll also create a counter/number for each institution per paper
#   group_by(id) %>%
#   mutate(author_num = row_number()) %>%
#   ungroup()
#   # We can also pivot to wide format if that makes sense at any point:
#   # pivot_wider(names_from = author_num,
#               # values_from = institution,
#               # names_prefix = "author_",
#               # values_fill = NA_character_)
# 
# View(data_locations) # Looks good!
# 
# # Uncomment from below this line
# 
# # Fortunately, the "maps"  package contains a list of city names that we can use to match with our author institutions. Let's load the relevant data:
# ## Loading country data from package maps
# data(world.cities)
# 
#  # The way the matching process works below is by picking the first match in the string, so removing all of the cities below actually leads to an increase in proper matches as the wrong matches are skipped over:
# world_cities_filtered <-  world.cities %>%
#   filter(!name %in% c("China", # There is a city in Mexico called China, and including this in the dataset needed to pick up any papers published in China and link them to this city!
#                         "India", # Same sort of issue (City in Africa)
#                         "San",  # Same sort of issue (City in Africa, again)
#                         "Institut", # This appears to be a City somewhere around Azerbaijan, but I think it's getting picked up as a city when in fact it just is a string in the author address referring to a university!
#                        "Santa", # This is picked up as a city In Peru, when in fact  with just part of the name of many different institutions
#                       "God", # This is picked up as the name of a city in the Hungary, when in fact is just  part of the name of a hospital in Ireland
#                       "Normal", # This is picked up as a city in the US  when in fact it's part of a university name in China
#                       "Bar", # This is picked up as a city in Ukraine, when in fact it's just the name of the University in Israel
#                       "Victoria", #  This leads to confusion between Victoria in Canada and the state in Australia. Easier just to remove rather than be inaccurate
#                       "Cardinal",
#                       "Villanueva", # This is actually a university in Spain, but it's picked up as a city in Honduras
# 
#                       "Beira", # This is picked up as a city of Mozambique, but eventually the University Hospital name in Portugal
#                       "Cardinal",
#                       "Young", # This is picked up as a city in Uruguay, when is just part of the name of a young adult hospital in France
# 
#                       "Cornwall", # Get picked up as a city in Canada and not the area of the UK
#                        "George", #  A street name in the US,  confused for the South African city
#                        "Imperial", #  part of a UK university name but picked up as a city in Peru
#                       "Laval", # Part of a university name in Canada but gets picked up as the city in France
#                       "Villa", # Part of the name of a institution in Italy, but picked up as the Estonian city.
#                       "Aidu", # Part of the name of a Japanese hospital, but picked up as a Estonian city
#                      "Carolina", # Part of the US state name in the data, but picked up as a city in Peurto Rico
#                        "Carmel", # Get confused with the US city, but always Israel in the dataset
#                               "U",#  seems like shorthand for an address in France, but has picked up as a city in Micronesia and the French city is missed
#                               "Ramon",  # Part of the name of a hospital in Spain, but picked up as the city in the Philippines
#                              "Fundacion", #  Part of the name of a institution in Spain, but picked up as a city in Colombia
#                              "Trinidad", # Part of a institution name in Argentina, were picked up as a city in Bolivia
#                              "Liege",# Picked up as a city in Belgium, but it's actually part of a name of a place in France
#                              "East London", # Refers to the UK University, picked up as a city in South Africa
#                              "Florida", # University name picked up as a city in Cuba
#                              "Ita", # Part of the name of a institution in Finland picked up as the city in Paraguay
#                      "Princeton", #  University name confused for the city in Canada
#                      "Humboldt", # Confused for the Canadian city, but actually a University in Germany
#                       "Alcala", # Confused for the Colombian city, but actually part of a university name in Spain)
#                             "York", # UK Canada confusion. Easier to remove
#                      "Union", # Typically refers to the European Union, but confuse for the US city
#                       "La Rioja", # Part of a Spanish University but computer the city in Argentina
#                      "Concord", # Area in Australia that ends up being linked to a US city rather
#                       "Nanyang", # Part of a Singapore university name that gets linked to a city in China
#                       "Patan", # Part of a Napoleon University name that gets linked to a cityin India
#                       "Saint-Joseph", # University name in Lebanon on that gets linked to a city in Reunion
#                      "Valencia", # University in Spain that gets linked to Venezuela
#                       "Ingenio", # Institution in Spain that gets linked to the Canary Islands
#                      "Lincoln", # UK university name that gets linked to the US
#                      "Roma", # Italian street name that gets linked to Australia
#                      "Leon", # Institution name in France forgets links to Mexico
#                      "Pau", # Part of a hospital name in Spain that Gets linked to France
#                      "Ilan", # Part of a university name in Israel that gets linked to Taiwan
#                      "Street", #  an obvious issue. Linked to the UK incorrectly
#                      "Alle",  # Incorrectly linked to Switzerland when it's an address in Denmark'Hashtag
#                      "San Ignacio", # Location improved its link to Bolivia
#                      "Carnot", # Street in France that gets linked to central Africa
#                       "Mexico", #  country gets incorrectly linked to the Philippines city
#                       "Mobile", #  Institution in Canada that gets incorrectly linked to the US
#                       "Hebron", # Location in Spain gets mixed up with Palestine
#                      "Liban", #  hospital in France that gets linked to Czech Republic
# 
#                      "Bayonne", # Addiction clinic in France linked to the US incorrectly
#                      "Apartado", # Confusion between Spain and Colombia. Better to just remove
#                      "Rioja", # Appears in a few different places and can be linked to Spain or Peru
#                       "Li", #  part of a university name in China can be linked to Norway incorrectly
#                      "Al", # Part of the name of places in Saudi Arabia and other countries that gets picked up as a city in Norway
#                      "San Agustin", # Part of the name of a place in Peru that getting linked to Mexico
#                      "Asia", # A city name in the Philippines that is obviously going to give problems
#                      "Jordan", #  country name acts incorrectly linked to the Philippines city
#                      "Kota", # Location in Malaysia that gets linked to India incorrectly
#                      "Ribera", # Part of an institution name in Spain gets linked to Italy
#                      "Pilar", # Name of a Institute in Croatia that gets linked to Brazil incorrectly
#                       "Greenwich", # Causes various problems due to being linked to the US and UK
#                       "George Town", #In Malaysia, baguettes linked to the Cayman Islands
#                      "Worth", # Should the Fort Worth in America, baguettes link to Germany
#                      "Santa Lucia", # Name of a institution in Italy that gets confused at the Canary Islands
#                      "Sainte Anne", # Name of an institution in France that gets confused with the city in Canada
#                      "Douglas", # Part of a university name inCanada gets confused with the Isle of Man
#                      "Arizona", # State name gets confused for a city in Honduras
#                      "Potsdam", # In New York gets confused with Germany
#                      "Kita", # In the name of an Indonesian institution that gets confused with the city in Mali
#                      "Concordia", # US university name gets confused with a town in Argentina
#                      "Bay", # Monterey Bay gets confused with a town in the Philippines
#                      "Parana", # Part of an institution name in Brazil gets linked to Argentina
#                      "Gazi", # Incorrectly gets linked to Ken year when it should be part of a name of a university in Turkey
#                      "Wufeng",# incorrectly linked to China when it should be in Taiwan
#                      "Loo", # Getting correctly linked to Estonia when it's actually part of a institution name in Singapore
#                      "Police", # Police college accidentally linked to city in Poland
#                      "Long", # Thia city and in the name of yale uni address
#                      "David", # City in Panama that causes obvious issues
#                      "Naval", # City in the Philippines that causes obvious issues
#                      "Hall", # City in the Philippines that causes obvious issues
#                      "Trinity" # Irish uni name but gets mistaken to Jersey city
#                      )) %>%
#   # A combination of city names and country names can be used to keep the city where it seems like it can be saved:
#     mutate(city_country = paste0(name,", ", country.etc)) %>%
#   filter(!city_country %in% c("Sussex, Canada",# Only the UK one appears and this gets confused
#                               "Milton, Canada", #  should be Milton Keynes in the UK
#                               "Bathurst, Canada", # Location in Australia they get confused Canada
#                               "Milton, New Zealand", #  should be Milton Keynes in the UK
#                               "Orleans, France", #  incorrectly linked to France, not Canada
#                               "Bergen, Norway", # teams to be linked consistently to the US,  but mistaken for Norway
#                               "Penrith, UK", # Should be the Australian city near Sydney
#                               "Bedford, UK", #  location in Australia gets Linked to the UK incorrectly
#                               "Bedford, USA", #  location in Australia gets Linked to the US incorrectly
#                               "Salt, Spain", # Should be Salt Lake City, US,  but gets picked up as the Spanish city
#                               "Lancaster, USA", #  should be the UK
#                               "Ho, Ghana", # Part of an address in Taiwan that's getting linked to Ghana
#                               "Laguna, USA", # Should be the canary islands, surprisingly
#                               "Albert, France",
#                               "Hong, Denmark",# Leads to this being picked up and said of Hong Kong
#                              "Durham, USA", # Should be the UK one
#                              "Brest, Belarus", # Refers to France not Belarus
#                              "Warwick, USA", #  Always seems to refer to the UK university, but the Use of the US city
#                              "Belmont, Canada", #  Always seems to refer to the US,  but confused for the Canadian city
#                              "Beaufort, Malaysia", # Should be the American city
#                              "Mackay, Australia", #  should always be the location in Taiwan
#                              "Alicante, Philippines", #  should always be in Spain
#                              "Malaya, Philippines", # Should be in Malaysia
#                            "Claremont, Jamaica", #  Australian location that gets sent to Jamaica, incorrectly
# 
#                            "Colombia, Cuba", # Get confused with the country
#                            "Carlton, UK", #Street name in Canada gets confused with the UK
#                            "Costa Rica, Mexico", # Obviously get complete. The country
#                            "Notre Dame, Mauritius", #  actually the Australian University!
#                             "Baja, Hungary", # University namely Mexico
#                            "Palmerston, Australia", #  links to the northern Australian city, rather than New Zealand
#                            "Waterloo, USA" # Always the Canadian university
#          )) %>%
#    group_by(name) %>%
#   filter(pop == max(pop)) %>% # Okay, so this is imperfect, but when a city name is duplicated that I haven't accounted for above, this will filter to select only the one with the highest population. This is based on the assumption that papers are likely to come from more populated cities (i.e. those with universities). This may seem crude, but it solved many, many issues in the map.
#   ungroup()
# 
# 
# # Extract just the city names so we can try and match author locations using these:
# city_names_from_world <- world_cities_filtered$name
# 
# # Create a pattern of city names for matching with word boundaries:
# city_names_pattern <- paste("\\b(", paste(city_names_from_world, collapse = "|"), ")\\b", sep = "")
# 
# 
# 
# # Extract city names using stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match we're looking for)
# cities_of_authors <- str_extract(data_locations$institution, city_names_pattern)
# 
# 
# 
# # Before we join this to our dataset, let's now do the same thing for countries, using the world dataset. We could just link countries to the existing cities we identified, but this won't give us the richest overall data, as in some cases it might find a country name but no city name and vice versa.
# 
# # Extract just the Country names so we can try and match author locations using these:
# country_names <- world.cities$country.etc
# 
# # Create a pattern of Country names for matching with word boundaries:
# country_names_pattern <- paste("\\b(", paste(country_names, collapse = "|"), ")\\b", sep = "")
# 
# # Extract country names using stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match we're looking for)
# countries_of_authors <- str_extract(data_locations$institution, country_names_pattern)
# 
# 
# 
# # Add the extracted city and country names to our dataset:
# data_locations_with_city_country<- data_locations %>%
#   bind_cols(cities_of_authors,
#             countries_of_authors) %>%
#   rename(cities_of_authors = 19,
#          countries_of_authors = 20)
# # Check everything looks okay:
# #  select(author_names,
# #         cities_of_authors,
# #         countries_of_authors) %>%
# #  print(n=150)
# 
# # Make the city name column consistent with paper dataset:
# world_cities_filtered <- as_tibble(world_cities_filtered) %>%
#   rename(cities_of_authors = name)
# 
# #Join the world.cities dataset with our paper data so we have latitude and longitude for each city:
# data_locations_with_full_geo_location<-
#   left_join(data_locations_with_city_country,
#            world_cities_filtered,
#            by = join_by(cities_of_authors) # This is done purposely, as I want to check whether the country names matched in text above match with the country names linked to the city names. Any mismatches tell us a lot about whether it got the right city are not and how I filtered out most of the problematic cities above!
#            )
# 
# # Now, Save this to a CSV file because this took forever to extract the data we don't want to have to do this every time I render this page!!
write.csv(data_locations_with_full_geo_location, "data_locations_with_full_geo_location.csv")

```

Phew, that was intense. It was computationally demanding to link \>40,000 author addresses to one out of every single city and country name worldwide so I've saved the dataset to Github and now I'll load it in from there and do some cleaning.

```{r results=FALSE}

# The code here is terrible. I'm sorry!
# Okay, now actually load pre-created data from Github:
url_geo_loc_data_link <- "https://raw.githubusercontent.com/rheirene/Quarto_Website/master/posts/2023-history-of-behavioural-addictions-PubMed-part2/data_locations_with_full_geo_location.csv"

data_locations_with_full_geo_location <- read_csv(url_geo_loc_data_link) %>%
  as_tibble() 

# Looking for mismatches between countries matched to cities I extracted and countries matched by text (as I do below) is how I spotted most of the problematic cities.

# names(data_locations_with_full_geo_location)

# Install proper spelling of names in world cities dataset:

world.cities %>%
  # filter(name == "Turin")
  filter(country.etc == "Canary Islands")
  
# Let's explore this data to see any mismatches
 data_locations_with_full_geo_location %>% 
filter(countries_of_authors != country.etc) %>% # Identify and isolate mismatches
  select(institution,
       author_names,
         cities_of_authors,
         countries_of_authors,
       country.etc) %>%
  print(n=350) # Started with several thousand
 
# Looking at the results above, it's clear we need to prioritise matched country (i.e., the one we extracted from the text; "countries_of_authors" over the linked one (i.e., linked to city name from database) as mistake are less common with the matched one as it's simple text extraction, whereas the link from city to country can be fallible as there are multiple cities with the same name and it could have matched with the wrong city.

# There are a few important caveats to the above that we will need to directly recode for accuracy:
# Georgia USA. This gets extracted as the country, but it's obviously the state in the US! Also, the city in Georgia, Athens gets links to Greece incorrectly. 
# Mongolia.  Inner Mongolia normal University appears to be in China, but gets extracted as in Mongolia 
# Mexico. The University of New Mexico often gets linked to the country Mexico when it should be the US
# Jersey. Gets linked to the small country south of the UK, instead of the University of New Jersey!
 
# There are also a few key cities that are duplicated that I didn't remove in the filtering above that will need directly linking via code to the right city and country, including Cambridge, New York, London, Oxford, Liverpool, Bristol, Reading, Columbia, Northampton, Stirling, Aberdeen and Newport

 # Check all potential problematic locations:
 data_locations_with_full_geo_location %>%
   # filter(countries_of_authors  == "Georgia") %>% 
   # filter(countries_of_authors  == "Mongolia") %>% 
   # filter(countries_of_authors  == "Mexico") %>% 
  # filter(countries_of_authors  == "Jersey") %>% 
   # filter(countries_of_authors  == "Jersey") %>% 
   # filter(str_detect(institution, "Columbia")) %>% # Check anywhere for this one!
  #  filter(str_detect(institution, "Colombia")) %>% # Check anywhere for this one!
  # filter(str_detect(cities_of_authors, "Marietta")) %>% 
   # filter(str_detect(cities_of_authors, "Liverpool")) %>%
   # filter(str_detect(cities_of_authors, "Reading")) %>%
   # filter(str_detect(cities_of_authors, "Northampton")) %>%
   # filter(str_detect(cities_of_authors, "Stirling")) %>%
   # filter(str_detect(cities_of_authors, "Cambridge")) %>% 
 # filter(str_detect(cities_of_authors, "Aberdeen")) %>%
 # filter(str_detect(cities_of_authors, "Newport")) %>%
  # filter(str_detect(cities_of_authors, "Oxford")) %>%
   # filter(str_detect(institution, "Kfar-Saba")) %>%
    filter(str_detect(institution, "Karsiyaka")) %>%
    select(institution,
       author_names,
         cities_of_authors,
         countries_of_authors,
       country.etc) %>%
  print(n=350)
 
data_locations_with_full_geo_location_cleaned <- 
 data_locations_with_full_geo_location %>%
  mutate(cities_of_authors = case_when(
    # Stop Bedford park being picked up as Canadian City
     str_detect(institution, "  Bedford Park, SA") ~ "Adelaide",
    # Sort Georgia country/Athens city issues:
    str_detect(institution, "Georgia State") ~ "Atlanta",
    str_detect(institution, "University of Georgia") ~ "Athens",
    str_detect(institution, "Georgia College") ~ "Milledgeville",
    #  Mongolia city issues:
    str_detect(institution, "Inner Mongolia") ~ "Hothot",
    # Mexico City issues:
    str_detect(institution, "University of New Mexico") ~ "Albuquerque",
    # Jersey issues:
    str_detect(institution, "Rutgers") &
    !str_detect(institution, "Camden") &
    !str_detect(institution, "New York") ~ "New Brunswick", # this isn't perfect, but cities are often missing for Rutgers
    # London city issues
     str_detect(institution, "University College London") ~ "London",
     str_detect(institution, "London, United Kingdom") ~ "London",
     str_detect(institution, "Grovelands Priory Hospital") ~ "London",
   # Sort Columbia issues:
    str_detect(institution, "Vancouver") ~ "Vancouver",
    str_detect(institution, "Columbia University") ~ "New York",
   # Saudi scholar:
    str_detect(institution, "Sami H. Alzahrani") ~ "Jeddah",
   
  # Sort Colombia issues:
    str_detect(institution, "Bogota") ~ "Bogota",
    str_detect(institution, "Barranquilla") ~ "Barranquilla",
    str_detect(institution, "Pasto") ~ "Pasto",
    str_detect(institution, "Campus Robledo") ~ "Medellin",
   # Cambridge city issues:
   str_detect(institution, "Cambridge University, UCL and NHS National Centre for gaming Disorders") ~ "London",
   # UBC & Canada other:
    str_detect(institution, "University of British Columbia") ~ "Vancouver",
   str_detect(institution, "Toronto, Canada") ~ "Toronto",
  str_detect(institution, "Department of Education, Centre for Addiction and Mental Health") ~ "Toronto",
    str_detect(institution, "Morton and Gloria Shulman Movement Disorders Clinic") ~ "Toronto",
          str_detect(institution, "University of New Brunswick") ~ "Fredericton",
  str_detect(institution, "Addiction & Mental Health Services-Kingston") ~ "Kingston",
  
   # Aberdeen issues:
   str_detect(institution, "Hong Kong") ~ "Hong Kong", # CHECK 312 and lower
   # Newport issues:
   str_detect(institution, "Christopher Newport University") ~ "Newport News",
   # Cyprus:
   str_detect(institution, "Karsiyaka") ~ "Karsiyaka",
   # Palo Alto
   str_detect(institution, "Palo Alto") ~ "Palo Alto",
   # UWV:
   str_detect(institution, "Morgantown") ~ "Morgantown",
   # Marid:
     str_detect(institution, "Madrid, Spain") ~ "Madrid",
  # Tours, France:
    str_detect(institution, "University of Tours") ~ "Tours",
   # Paris:
    str_detect(institution, "Pole paris 12") ~ "Paris",
   str_detect(institution, "Paris, France") ~ "Paris",
  str_detect(institution, "Centre Pierre Nicole") ~ "Paris",
    str_detect(institution, "centre hospitalier Sainte-Anne") ~ "Paris",
  # South Korea:
  str_detect(institution, "Seoyoung University") ~ "Paju-si",
  
  # Providence:
      str_detect(institution, "Brown University") ~ "Providence",
  # North Kingston, RI, USA
      str_detect(institution, "North Kingston") ~ "Providence",
  # Essesx uni campus:
  str_detect(institution, "University of Essex, Colchester") ~ "Colchester",
  # Serbia uni & Belgrade:
      str_detect(institution, "Novi Pazar, Serbia") ~ "Novi Pazar",
    str_detect(institution, "Belgrade, Serbia") ~ "Belgrade",
  # Milan:
   str_detect(institution, "Milan, Italy") ~ "Milan",
  # UMissouri:
   str_detect(institution, "University of Missouri") ~ "Columbia",
  # Rochester:
   str_detect(institution, "Rochester") ~ "Rochester",
  # Bedford, USA:
  str_detect(institution, "Edith Nourse Rogers Memorial Hospital") ~ "Bedford",
# Yale:
  str_detect(institution, "New Haven, CT") ~ "New Haven",
  str_detect(institution, "Yale University") ~ "New Haven",
# Auckland & Newzeland:
  str_detect(institution, "Auckland") ~ "Auckland",
 str_detect(institution, "Palmerston North, Manawatu") ~ "Palmerston North",
# Netherlands uni confused for Austria:
str_detect(institution, "Instituut voor Onderzoek") ~ "Netherlands", 
# Lausanne:
  str_detect(institution, "Lausanne") ~ "Lausanne",
# Boston issues:
  str_detect(institution, "Boston, MA") ~ "Boston",
  str_detect(institution, "Veteran's MH and Addiction Program, VA") ~ "Boston",
  str_detect(institution, "Berenson-Allen Center for Noninvasive") ~ "Boston",
# New york:
   str_detect(institution, "Nassau Community College") ~ "New York",
str_detect(institution, "St Bonaventure University") ~ "New York",
str_detect(institution, "Elmhurst Hospital Center") ~ "New York",
str_detect(institution, "Cure Huntington's Disease Initiative") ~ "New York",
# Case Western Reserve University: 
str_detect(institution, "Case Western Reserve University") ~ "Cleveland", # Goes to Bolton UK

# Amityville
str_detect(institution, "Amityville") ~ "Amityville",
# German city:
   str_detect(institution, "Villingen-Schwenningen") ~ "Villingen-Schwenningen",
  str_detect(institution, "Hurth, Germany") ~ "Hurth",
str_detect(institution, "Martin-Luther-University") ~ "Halle",
# Iraninan city:
  str_detect(institution, "Isfahan, Iran") ~ "Isfahan",
  str_detect(institution, "Aja University") ~ "Tehran",
# Israel:
str_detect(institution, "Kfar-Saba") ~ "Tel Aviv",
str_detect(institution, "Leslie and Susan Gonda") ~ "Tel Aviv",	

# South Korea:
str_detect(institution, "Hanyang University") ~ "Seoul",
str_detect(institution, "Chungmugong Leadership Center") ~ "Changwon",
str_detect(institution, "Korea Institute on Behavioral Addictions") ~ "Seoul",
str_detect(institution, "hallym University") ~ "Anyang",
# Christiana Care Hospital:  
str_detect(institution, "Christiana Care Hospital") ~ "Wilmington", 
# Rush University Medical Center:
str_detect(institution, "Rush University Medical Center") ~ "Chicago",
 # Jordan city:
 str_detect(institution, "Amman 19392, Jordan") ~ "Amman", 
# Perth/Aus:
 str_detect(institution, "Perth, Australia") ~ "Perth", 
 str_detect(institution, "Adelaide") ~ "Adelaide", 
str_detect(institution, " CQUniversity, 400 Kent St, Sydney") ~ "Sydney", 
# Oxford Uni:
 str_detect(institution, "Oxford, United Kingdom") ~ "Oxford", 
# Salford Uni:
 str_detect(institution, "Frederick Road Campus") ~ "Salford", 
 # Italian cities:
 str_detect(institution, "Portici, Italy") ~ "Naples", 
str_detect(institution, "Betania Evangelical Hospital") ~ "Naples", 
 str_detect(institution, "Genoa, Italy") ~ "Genoa", 
 str_detect(institution, "Lecco, Italy") ~ "Lecco", 
str_detect(institution, "University of Genova") ~ "Genoa", 
str_detect(institution, "Terni, Italy") ~ "Terni", 
str_detect(institution, "Telese Terme") ~ "Telese",
str_detect(institution, "Urbino") ~ "Urbino", 
str_detect(institution, "Universita Cattolica") ~ "Rome", 
str_detect(institution, "University of Turin, Torino") ~ "Turin", 
str_detect(institution, "Alma Mater Studiorum") ~ "Bologna", 


  # barcelona & other spanish city issues:
  str_detect(institution, "Barcelona, Spain") ~ "Barcelona",
  str_detect(institution, "Universitat Rovira i Virgili") ~ "Tarragona",
  str_detect(institution, "Alicante, Spain") ~ "Alicante",
  str_detect(institution, "Bormujos, Spain") ~ "Bormujos",
str_detect(institution, "Ciencies d'Alimentacio") ~ "Barcelona",
str_detect(institution, "Jimenez Diaz University Hospital") ~ "Mardrid",
str_detect(institution, "Centro Universitario Cardenal Cisneros") ~ "Madrid", 
str_detect(institution, "Santiago de Compostela") ~ "Santiago de Compostela", 
str_detect(institution, "Consumer and User Psychology Unit, Faculty of Psychology, University of Santiago") ~ "Santiago de Compostela", 
str_detect(institution, "Department of Physiology, School of Medicine, University of Santiago de") ~ "Santiago de Compostela", # Confirmed by searching original paper
str_detect(institution, "Serra Hunter Programme") ~ "Barcelona",
str_detect(institution, "Hospital Universitario de Canarias") ~ "Santa Cruz de Tenerife",
str_detect(institution, "Universitat Pompeu Fabra") ~ "Barcelona",
str_detect(institution, "Universidad Loyola Andalucia") ~ "Cordoba",

# Nottingham:
 str_detect(institution, "Newark Beacon Innovation Centre") ~ "Nottingham",

# Portugal:
str_detect(institution, "Unity in Multidisciplinary Research on Biomedicine (UMIB)") ~ "Porto",
str_detect(institution, "IAJ (Gambling Support Institute)") ~ "Lisbon",
# Turkey:
str_detect(institution, "Inonu University") ~ "Malatya",
  # Porto Alegre issue:
  str_detect(institution, "Porto Alegre") ~ "Porto Alegre",
# Namur:
 str_detect(institution, "Namur") ~ "Namur", # Checked there's only one in the data
# Sandy bay tas:
 str_detect(institution, "Sandy Bay") ~ "Sandy Bay",
# Oviedo:
 str_detect(institution, "Oviedo, Spain") ~ "Oviedo",
 str_detect(institution, "Virginia Tech, Blacksburg, VA") ~ "Roanoke",
# Dublin & ireland:
 str_detect(institution, "Lucena Clinic Rathgar") ~ "Dublin",
 str_detect(institution, "Dublin, Ireland") ~ "Dublin",
 # West Chester University:
   str_detect(institution, "West Chester University") ~ "Philadelphia",
  # Yunlin and others, Taiwan:
  str_detect(institution, "Yunlin, Taiwan") ~ "Douliu",
str_detect(institution, "Chang Gung Memorial Hospital") ~ "Taoyuan City",
    # University of Montana:
  str_detect(institution, "University of Montana") ~ "Missoula",
      # San Juan:
  str_detect(institution, "University of Puerto Rico, San Juan") ~ "San Juan",
        # Moroccan city:
    str_detect(institution, "Fez, Morocco") ~ "Fez",
  # Seton Hall University & other newark issues:
  str_detect(institution, "Seton Hall University") ~ "Newark",
    str_detect(institution, "Parsippany, NJ") ~ "Newark",
# Auburn University:
str_detect(institution, "Auburn University") ~ "Montgomery",
#  University of Manitoba:
str_detect(institution, "University of Manitoba") ~ "Winnipeg",
# University of South Dakota:
str_detect(institution, "University of South Dakota") ~ "Vermillion",
# University of Antwerpen:
str_detect(institution, "Institute Born-Bunge") ~ "Antwerp",
# University of Kansas:
str_detect(institution,  "University of Kansas") ~ "Lawrence",
# Vanderbilt University:
str_detect(institution,  "Vanderbilt") ~ "Nashville",
# Wayne state university:
str_detect(institution,  "Wayne State University") ~ "Detroit",
# University  Michigan:
str_detect(institution,  "University of Michigan") ~ "Ann Arbor",
# Carson College of Business:
str_detect(institution,  "Carson College of Business") ~ "Pullman",
# Universidad de San Martin de Porres
str_detect(institution,  "Universidad de San Martin de Porres") ~ "Lima",
# The Center for Internet and Technology Addiction
str_detect(institution,  "The Center for Internet and Technology Addiction") ~ "Hartford",
  # Georgetown uni issue:
  str_detect(institution, "Georgetown") ~ "Washington, D.C",
  # Lincoln Memorial University issue:
  str_detect(institution, "Lincoln Memorial University") ~ "Harrogate",
  # Meridian Behavioral Health Services (I can't find a full address for this, despite searching the people manually, But it doesn't appear to be in the US):
  str_detect(institution,  "Meridian Behavioral Health Services") ~ "",
  # Brazilian hospital:
     str_detect(institution, "Belo Horizonte") ~ "Brazil",
  # Nashville:
    str_detect(institution, "Nashville, TN") ~ "Nashville",
    # Fort Lauderdale:
    str_detect(institution, "Fort Lauderdale") ~ "Fort Lauderdale",
      # Byblos Lebanon:
    str_detect(institution, "Byblos, Lebanon") ~ "Byblos",
 # Belgium city:
    str_detect(institution, "Hasselt, Belgium") ~ "Hasselt",
# Poland:
 str_detect(institution, "Uniwersytet Jagiellonski Collegium Medicum") ~ "Krakow",
 str_detect(institution, "Adam Mickiewicz University") ~ "Poznan",
 # Peru:
  str_detect(institution, "Lima, Peru") ~ "Lima",
#Japan:
str_detect(institution, "Konan Women's University") ~ "Kobe",
str_detect(institution, "Tokai Gakuen University") ~ "Tokai Gakuen University",
str_detect(institution, "University of Hokkaido") ~ "Hokkaido",
str_detect(institution, "Tama-ku, Kawasaki") ~ "Kawasaki",

# Virginia:
 str_detect(institution, "Virginia Institute for Psychiatry and Behavioral Genetics") ~ "Richmond",
# Kuala Lumpur:
 str_detect(institution, "Monash University Malaysia") ~ "Kuala Lumpur",
 # Vietnam:
 str_detect(institution, "Nguyen Tat Thanh University") ~ "Ho Chi Minh City",
   # Santo AndrÃ©:
    str_detect(institution, "Santo Andre, SP, Brazil") ~ "Santo Andre",
# author not institution:
str_detect(institution, "Maya Sahu, RN, RM,") ~ "",
    TRUE ~ as.character(cities_of_authors)  # Default if none of the above matches

 # COUNTRY CHANGES:
 )) %>%
  mutate(countries_of_authors = case_when(
    # France: 
      # Tours, France:
    str_detect(institution, "University of Tours") ~ "France",
    # Poland: 
     str_detect(institution, "Adam Mickiewicz University") ~ "Poland",
 # Sort Georgia country issues:
    str_detect(institution, "Georgia State University") ~ "USA",
    str_detect(institution, "University of Georgia") ~ "USA",
    str_detect(institution, "Georgia College") ~ "USA",
    str_detect(cities_of_authors, "Marietta") ~ "USA",
  # Georgetown uni issue:
  str_detect(institution, "Georgetown") ~ "USA",
   # Lincoln Memorial University issue:
  str_detect(institution, "Lincoln Memorial University") ~ "USA",
  # Mongolia country issues:
    str_detect(institution, "Inner Mongolia") ~ "China",
  # Mexico Country issues:
    str_detect(institution, "University of New Mexico") ~ "USA",
  # Germany issues:
  str_detect(institution, "Martin-Luther-University") ~ "Germany",
  # Jersey country issues:
    str_detect(institution, "New Jersey") ~ "USA",
  # Cambridge city issues:
    str_detect(institution, "Harvard") ~ "USA",
    str_detect(institution, "University of Cambridge") ~ "UK",
    str_detect(institution, "Cambridge Health Alliance") ~ "USA",
    str_detect(institution, "Anglia Ruskin University") ~ "UK",
  # Boston issue:
    str_detect(institution, "Boston, MA") ~ "USA",
  # London city issues
    str_detect(institution, "London")  &
    !str_detect(institution, "Ontario") &
    !str_detect(institution, "Canada") ~ "UK",
     str_detect(institution, "Ontario") ~ "Canada",
  # Sort Columbia issues:
  
    str_detect(institution, "University of British Columbia") ~ "Canada",
    str_detect(institution, "Columbia University") ~ "USA",
    str_detect(institution, "United States of America") ~ "USA",
    str_detect(institution, "Missouri") ~ "USA",
    str_detect(institution, "New York") ~ "USA",
    str_detect(institution, "British Columbia") ~ "Canada",
    str_detect(institution, "Chilliwack") ~ "Canada",
  str_detect(institution, "Centro Universitario Cardenal Cisneros") ~ "Spain",
  # Saudi scholar:
    str_detect(institution, "Sami H. Alzahrani") ~ "Saudi Arabia",
 
  # Australia:
  str_detect(institution, "Adelaide") ~ "Australia",
  str_detect(institution, " CQUniversity, 400 Kent St, Sydney") ~ "Australia",
  str_detect(institution, " CQUniversity") ~ "Australia",
  # Austria:
   str_detect(institution, "Therapiestation Lukasfeld der Stiftung Maria, Ebene") ~ "Austria",
   # Sort Colombia issues:
     str_detect(institution, "Barranquilla") ~ "Colombia",
  # Canada:
    str_detect(institution, "Department of Education, Centre for Addiction and Mental Health") ~ "Canada",
      str_detect(institution, "Morton and Gloria Shulman Movement Disorders Clinic") ~ "Canada",
        str_detect(institution, "University of New Brunswick") ~ "Canada",
    str_detect(institution, "Addiction & Mental Health Services-Kingston") ~ "Canada",
  str_detect(institution, "Toronto, ON") ~ "Canada",
  str_detect(institution, "Social and Economic Impacts of Gambling in Massachusetts project,") ~ "USA", # Was originally Canada
  # Case Western Reserve University: 
str_detect(institution, "Case Western Reserve University") ~ "USA", # Goes to Bolton UK

  # South Korea:
str_detect(institution, "Hanyang University") ~ "South Korea",
str_detect(institution, "Chungmugong Leadership Center") ~ "South Korea",
str_detect(institution, "Korea Institute on Behavioral Addictions") ~ "South Korea",
str_detect(institution, "hallym University") ~ "South Korea",
str_detect(institution, "Seoyoung University") ~ "South Korea",
# Portugal:
str_detect(institution, "Unity in Multidisciplinary Research on Biomedicine (UMIB)") ~ "Portugal",
  # China:
    str_detect(institution, "The Chinese University of Hong Kong") ~ "China",

# South Africa:
str_detect(institution, "Christiana Care Hospital") ~ "USA", 

# Newark Beacon Innovation Centre:
 str_detect(institution, "Newark Beacon Innovation Centre") ~ "UK",

  # Sort Liverpool issues: 
  str_detect(institution, "John Moores") ~ "UK",
  str_detect(institution, "University of Liverpool") ~ "UK",
  str_detect(institution, "LiMRIC") ~ "UK",
  str_detect(institution, "Liverpool, England") ~ "UK",
   str_detect(institution, "Liverpool John") ~ "UK",
  # Reading issues:
  str_detect(institution, "University of Reading") ~ "UK",
   str_detect(institution, "USA") ~ "USA",
  str_detect(institution, "United States") ~ "USA",
  # Northampton issues:
  str_detect(institution, "University of Northampton") ~ "UK",
  str_detect(institution, "Gemini Research") ~ "USA",
  # Virginia:
 str_detect(institution, "Virginia Institute for Psychiatry and Behavioral Genetics") ~ "USA",
 # University of Alabama:
   str_detect(institution, "University of Alabama") ~ "USA",
# University  Michigan:
str_detect(institution,  "University of Michigan") ~ "USAr",
 # The Center for Internet and Technology Addiction
str_detect(institution,  "The Center for Internet and Technology Addiction") ~ "USA",
#  University of Southern  California:
str_detect(institution,  "University of Southern California") ~ "USA",
# Salford Uni:
 str_detect(institution, "Frederick Road Campus") ~ "UK", 
  # Sterling issues:
   str_detect(institution, "Australia") ~ "Australia",
   # Palo Alto
   str_detect(institution, "Palo Alto") ~ "USA",
   # Aberdeen issues:
   str_detect(institution, "Hong kong") ~ "China",
  # Newport issues:
   str_detect(institution, "Christopher Newport University") ~ "USA",
  # Oxford issues:
   str_detect(institution, "University of Oxford") ~ "UK", 
  str_detect(institution, "Oxford, UK") ~ "UK", 
  str_detect(institution, "Oxford Centre for") ~ "UK",
  # Weird barcelona uni & spain issue:
  str_detect(institution, "University of Barcelona, Barcelona") ~ "Spain",
  str_detect(institution, "Jimenez Diaz University Hospital") ~ "Spain",
str_detect(institution, "Santiago de Compostela") ~ "Spain", 
str_detect(institution, "Consumer and User Psychology Unit, Faculty of Psychology, University of Santiago") ~ "Spain", 
str_detect(institution, "Serra Hunter Programme") ~ "Spain",
str_detect(institution, "Hospital Universitario de Canarias") ~ "Spain",
str_detect(institution, "Universitat Pompeu Fabra") ~ "Spain",
str_detect(institution, "Universidad Loyola Andalucia") ~ "Spain",
# Italy:
str_detect(institution, "Urbino") ~ "Italy", 
str_detect(institution, "Betania Evangelical Hospital") ~ "Italy", 
str_detect(institution, "University of Turin, Torino") ~ "Italy", 
str_detect(institution, "Alma Mater Studiorum") ~ "Italy", 

# Brazil:
str_detect(institution, "Porto Alegre") ~ "Brazil", 
# Netherlands uni confused for Austria:
str_detect(institution, "Instituut voor Onderzoek") ~ "Netherlands", 

  # Lausanne issues:
 str_detect(institution, "Lausanne") ~ "Switzerland",
 # West Chester University:
   str_detect(institution, "West Chester University") ~ "USA",
#Rede SARAH de Hospitais de Reabilitacao:
   str_detect(institution, "Rede SARAH de Hospitais de Reabilitacao") ~ "Brazil",
 # Taiwan:
    str_detect(institution, "Taichung") ~ "Taiwan",
str_detect(institution, "Chang Gung Memorial Hospital") ~ "Taiwan",
# Turkey:
str_detect(institution, "Inonu University") ~ "Turkey",
  # Vietnam:
 str_detect(institution, "Nguyen Tat Thanh University") ~ "Vietnam",
 # Spain:
 str_detect(institution, "Ciencies d'Alimentacio") ~ "Spain",
# Iran:
 str_detect(institution, "Aja University") ~ "Iran",
# Rush University Medical Center:
str_detect(institution, "Rush University Medical Center") ~ "USA",
# Belgium:
str_detect(institution, "Institute Born-Bunge") ~ "Belgium",
# Peru:
# Universidad de San Martin de Porres
str_detect(institution,  "Universidad de San Martin de Porres") ~ "Peru",
# Israel:
 str_detect(institution, "Beit-Berl College") ~ "Israel",
str_detect(institution, "Leslie and Susan Gonda") ~ "Israel",

# author not institution:
str_detect(institution, "Maya Sahu, RN, RM,") ~ "",
    TRUE ~ as.character(countries_of_authors)  # Default if none of the above matches
  ))
 
# Now look to see what's left and see if it matters now if we assume our created city and country names (i.e., cities_of_authors AND countries_of_authors) are correct and the link (country.etc) is an error:
 data_locations_with_full_geo_location_cleaned %>% 
filter(countries_of_authors != country.etc) %>% # Identify and isolate mismatches
  select(institution,
       author_names,
         cities_of_authors,
         countries_of_authors,
       country.etc) %>%
  print(n = 340) # Okay, happy theses are good (after 10+ rounds of filtering)

 
 
# HERE IS WHERE YOU"RE REALLY AT NOW ROB:
 
 
# Now look at papers where we couldn't match the country but linking the world.cities dataset connects a country with a city. Error check this:
  data_locations_with_full_geo_location_cleaned %>% 
 filter(is.na(countries_of_authors) & !is.na(country.etc)) %>% # Identify and isolate these instances
    # Filter defined only unique instances to save time: 
     distinct(cities_of_authors, .keep_all = TRUE)  %>% 
  select(institution,
         cities_of_authors,
         countries_of_authors,
       country.etc) %>%
    # View()
  print(n = 1000) 

  
 # data_locations_with_full_geo_location_cleaned %>% 
#    filter(str_detect(institution, "arson College of Business")
 #          ) %>% 
    # View()
# Okay,  I've gone back to the above recording section to correct any country mismatches found in the above output:

  
# need to try and linkj long and lat to the combination of city and country where possible

 
 # MAKE GIT FILE SMALLER FOR STORAGE

 # Other problematic  Cities we need to directly code:
 # Athens
 # Nottingham
```

# Data summaries

Provide some basic data summaries of interest that will be of use in the paper or as supplementaal information.

### No. unqiue studies
How many articles are there in total in the sample, how many unique articles are there, and what is the discrepancy between these numbers?
```{r include=FALSE}
data %>%
  distinct(PMID) %>%
  nrow() %>%
  as_tibble() %>%
  mutate(full_count = nrow(data)) %>%
  mutate(full_unique_discrepancy = full_count-value) %>%
  rename(unique_count = value) %>%
gt()

# data %>%
#   distinct(PMID, .keep_all = TRUE) %>%
#   group_by(Label) %>% 
#   filter(Year %in% c(min(Year), max(Year)))


```

### Range of years for each addiction

That was the first and last years of publication for each addiction:
```{r}
# Find the first and last study for each "addiction" with number of papers at these years:
data %>%
  distinct(PMID, .keep_all = TRUE) %>%
  group_by(Label) %>% 
  summarise(
    min = min(Year), # identify first year
    max = max(Year) # identify last  year
  ) %>% 
  arrange(min) %>%
gt()
```


