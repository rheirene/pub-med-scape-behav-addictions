str_detect(institution, "Rush University Medical Center") ~ "USA",
# Belgium:
str_detect(institution, "Institute Born-Bunge") ~ "Belgium",
# Peru:
# Universidad de San Martin de Porres
str_detect(institution,  "Universidad de San Martin de Porres") ~ "Peru",
# Israel:
str_detect(institution, "Beit-Berl College") ~ "Israel",
str_detect(institution, "Leslie and Susan Gonda") ~ "Israel",
# author not institution:
str_detect(institution, "Maya Sahu, RN, RM,") ~ "",
TRUE ~ as.character(countries_of_authors)  # Default if none of the above matches
))
# Let's explore this data to see any mismatches
data_locations_with_full_geo_location_cleaned %>%
filter(countries_of_authors != country.etc) %>% # Identify and isolate mismatches
select(institution,
author_names,
cities_of_authors,
countries_of_authors,
country.etc) %>%
print(n=350) # Started with several thousand
# Now look to see what's left and see if it matters now if we assume our created city and country names are correct and the link is an error:
data_locations_with_full_geo_location_cleaned %>%
filter(countries_of_authors != country.etc) %>% # Identify and isolate mismatches
select(institution,
author_names,
cities_of_authors,
countries_of_authors,
country.etc) %>%
print(n = 340) # Okay, happy theses are good (after 10+ rounds of filtering)
# Now look at papers where we couldn't match the country but linking the world.cities dataset connects a country with a city. Error check this:
data_locations_with_full_geo_location_cleaned %>%
filter(is.na(countries_of_authors) & !is.na(country.etc)) %>% # Identify and isolate these instances
# Filter defined only unique instances to save time:
distinct(cities_of_authors, .keep_all = TRUE)  %>%
select(institution,
cities_of_authors,
countries_of_authors,
country.etc) %>%
# View()
print(n = 1000)
# Chunk 1
# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install
# Load in packages using `groundhog` to ensure consistency of the versions used here:
library(groundhog) # Load
# set.groundhog.folder("C:/Users/rhei4496/Groundhog packages") # Set in a writable directory
get.groundhog.folder()
# List desired packages:
packages <- c("tidyverse",
'readr', # Load dataset from GitHib
'RCurl', # Load dataset from GitHib
'gtExtras', # Add colours to gt tables
'transformr', # Needed for certain animations (dumbell lines)
'png',# Helps render gganimate plots
'gifski', # Helps render gganimate plots
'rmarkdown', # Helps render gganimate plots
'av', # render gganimate plots as videos
'Cairo', # Anti-aliasing for the line plots (smoothing output)
'ggtext', # make fancy labels in plots
'sysfonts', # Special fonts for figures
'showtext', # Special fonts for figures
'scico', # Colour palette
'maps', # Get map/geographic data for author locations
'purrr', # Help unnest city and author names across papers  equally
'kableExtra', # Make tabless
'formattable', #  Add visualisations to tables
'gt', # Alternative table options
'gtsummary', # Create summary tables
'scales', # Allows for the removal of scientific notation in axis labels
'ggrain', # Make rain cloud plots
'waffle', # make waffle plots for proportions
'networkD3', # Make Sankey plots to show relationships
'patchwork', # Join plots in multipanel layouts
'pwr', # Check statistical power
'car', # Perform ANCOVA stats tests
'rstatix', # Perform ANCOVA stats tests
'ggpubr', # Plots for linearity checks
'broom', # Print summaries of statistical test outputs
'psych', # get detailed summary figures to Supplement statistical tests
'ggstatsplot', # Plots with statistical outputs
'janitor', # Make column names consistent format
'caret', # Compute model performance indices
'sessioninfo', # Detailed session info for reproducibility
"osfr",
"readxl",
# "Gmisc", # Produce prisma flow diagram
# 'grid', # Produce prisma flow diagram
# "glue", # Produce prisma flow diagram
"apa", # print test results in apa format
"apaTables", # print test results in apa format
"ggh4x", # truncate graph axis lines
"truncnorm", # Generate normally distributed data with limits
"ComplexUpset" # produce upset plots
)
# Load desired package with versions specific to project start date:
groundhog.library(packages, "2024-05-30")
# force.install=TRUE)
# tolerate.R.version = '4.4.0')
# groundhog.library(packages, "2023-12-29")
# Chunk 2
## Setup presentation & graph specifications. Set up a standard theme for plots/data visualisations:
# Load new font for figures/graphs
font_add_google("Poppins")
font_add_google("Reem Kufi", "Reem Kufi")
font_add_google("Share Tech Mono", "techmono")
windowsFonts(`Segoe UI` = windowsFont('Segoe UI'))
showtext_auto()
showtext_auto(enable = TRUE)
# Save new theme for figures/graphs.This will determine the layout, presentation, font type and font size used in all data visualisations presented here:
plot_theme<- theme_classic() +
theme(
text=element_text(family="Poppins"),
plot.title = element_text(hjust = 0.5, size = 16),
plot.subtitle = element_text(hjust = 0.5, size = 13),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12),
plot.caption = element_text(size = 12),
legend.title=element_text(size=12),
legend.text=element_text(size=10)
)
# Chunk 3
# Now I'll load in the dataset and do a little cleaning. Of note, I'm going to remove all publications from 2023 so that we only have data for complete years (see comments in the code chunk below for any other exclusions).
#| code-fold: true
#| code-summary: "Code: load dataset"
url_behav_addic_data_link <- "https://raw.githubusercontent.com/rheirene/pub-med-scape-behav-addictions/main/Data%20extraction/combined_results_clean.csv"
raw_data <- read_csv(url_behav_addic_data_link) %>%
as_tibble()
str(raw_data)
# Chunk 4
filtered_data <- raw_data %>%
filter(str_detect(Publication_Type, "Erratum") |
str_detect(Publication_Type, "corrigendum") |
str_detect(Publication_Type, "Retraction")) %>%
distinct(PMID, .keep_all = TRUE)
# Let's now remove these pubs and any from 2023 so we have data for all "full" years:
data <- raw_data %>%
anti_join(filtered_data)
# View(data)
# Chunk 5
data %>%
count(Language) %>%
arrange(desc(n)) %>%
gt()
# Chunk 6
language_map <- c(
"eng" = "English", "ger" = "German", "fre" = "French", "rus" = "Russian",
"spa" = "Spanish", "hun" = "Hungarian", "jpn" = "Japanese", "chi" = "Chinese",
"pol" = "Polish", "ita" = "Italian", "kor" = "Korean", "cze" = "Czech",
"por" = "Portuguese", "heb" = "Hebrew", "dut" = "Dutch", "gre" = "Greek",
"swe" = "Swedish", "fin" = "Finnish", "nor" = "Norwegian", "dan" = "Danish",
"srp" = "Serbian", "tur" = "Turkish", "hrv" = "Croatian", "lit" = "Lithuanian",
"eng; spa" = "English; Spanish", "spa; eng" = "English; Spanish",
"por; eng" = "English; Portuguese", "eng; por" = "English; Portuguese",
"eng; jpn" = "English; Japanese", "jpn; eng" = "English; Japanese",
"eng; pol" = "English; Polish", "pol; eng" = "English; Polish",
"eng; tur" = "English; Turkish", "tur; eng" = "English; Turkish",
"eng; chi" = "English; Chinese",
"eng; fre" = "English; French",
"eng; ger" = "English; German",
"por; spa; eng"	= "Portuguese; Spanish"
)
data_w_languages <- data %>%
mutate(Language_recoded = language_map[Language])
data_w_languages %>%
distinct(PMID, .keep_all = TRUE) %>%
count(Language,
Language_recoded) %>%
arrange(desc(n)) %>%
gt()
# Chunk 7
data_w_languages %>%
distinct(Publication_Type) %>%
nrow()
# Chunk 8
data_w_languages %>%
count(Publication_Type ) %>%
arrange(desc(n)) %>%
gt()
# Chunk 9
data_w_languages %>%
filter(str_detect(Publication_Type, "Retracted")) %>%
distinct(PMID, .keep_all = TRUE) %>%
group_by(Label) %>%
count() %>%
arrange(desc(n))
# Chunk 10
data_w_languages_pub_types <- data_w_languages %>%
mutate(Publication_Category = case_when(str_detect(Publication_Type, "Case Reports") ~ "Case Report(s)",
(str_detect(Publication_Type, "Comment") |
str_detect(Publication_Type, "commentary") |
str_detect(Publication_Type, "Editorial") |
str_detect(Publication_Type, "Letter")) ~ "Editorial, Letter, or Comment",
(str_detect(Publication_Type, "Systematic Review") |
str_detect(Publication_Type, "Meta-Analysis") |
str_detect(Title, "Systematic Review")|
str_detect(Title, "Systematic review")|
str_detect(Title, "systematic review")|
str_detect(Title, "Meta-Analysis")|
str_detect(Title, "Meta-analysis")|
str_detect(Title, "meta-analysis")) ~ "Systematic Review/ Meta-Analysis",
(str_detect(Publication_Type, "; Review") |
str_detect(Title, "Review")) ~ "Review",
(str_detect(Publication_Type, "Controlled Trial") |
str_detect(Publication_Type, "controlled trial") |
str_detect(Publication_Type, "Clinical Trial")|
str_detect(Publication_Type, "Clinical trial")|
str_detect(Publication_Type, "clinical trial")|
str_detect(Title, "Controlled Trial")|
str_detect(Title, "controlled trial")|
str_detect(Title, "Randomised Controlled Trial")|
str_detect(Title, "randomised controlled trial")|
str_detect(Title, "Randomised Clinical Trial")|
str_detect(Title, "randomised clinical trial")
) ~ "Controlled Trial Report or Protocol",
TRUE ~ "Journal Article (non-specified)"))
# For checking strings:
# data_w_languages %>%
#  filter(str_detect(Title, "Clinical Trial")) %>%
#   View()
# Check coding through random selections of articles:
# data_w_languages_pub_types %>%
#   select(Title,
#          Publication_Type,
#          Publication_Category) %>%
#   slice(300:400) %>% # Alter to vary :)
# print(n=100)
# Check coding through specific new category:
# data_w_languages_pub_types %>%
#   select(Title,
#          Publication_Type,
#          Publication_Category) %>%
#   filter(Publication_Category == "Controlled Trial Report or Protocol") %>% # Alter to vary :)
#   print(n=100)
# Chunk 11
data_w_languages_pub_types %>%
distinct(PMID, .keep_all = TRUE) %>%
# group_by(Label) %>%
count(Publication_Category) %>%
mutate(Percent = round(n/sum(n)*100,2)) %>%
arrange(desc(n)) %>%
gt()
# Chunk 12
#| code-fold: true
#| code-summary: "Code: Extract location data for each paper"
# ***********************The below code is almost all commented out on purpose as the process of extracting and matching city names from the author address column is so computationally taxing that it takes a long time to process.  I've left the code here so that anyone can see how I did it, but I saved the results as a .csv file and now load the data like that***********************
# # as_tibble(data$Author_Address) # Take a look at how the author addresses are structured
#
# # Okay, so we're going to need to create an ID variable for each paper (this makes the string split and a nest below work bettter than relying on titles), then split the author_address strings into separate addresses, then unnest these into new rows.
#
# # Whilst I'm splitting and unnesting the author address: I'm going to simultaneously do this for the authors name column.
#
#
# # The un-nest doesn't seem to work well when we retain all of the columns in the dataset, so I do it with only id and institution (address) in the data, then join all of the rest of the data set to the unnested rows after this. For this to work, we need to create a dataset that has the ID variable in before splitting the string and and unnesting. Let's do that:
#  data_id <-data %>%
#   rowid_to_column(var = "id")
#
#  pad_vector <- function(vec, len) {
#   length(vec) <- len
#   return(vec)
#  }
#
# # Now split the author and address strings and then  unnest it into multiple rows, and finally re-join with the main dataset
# data_locations <- data_id %>%
#     mutate(
#      institution = str_split(Author_Address, ";"), # Split author address column into separate strings For each address
#     author_names = str_split(Full_Author_Name, ";") # Split author name column into separate strings For each Name
#   ) %>%
#   # The below code matches the number of author institutions and author names where discrepancies exist, so the unnest further below works:
#    mutate(
#     max_length = pmax(map_int(institution, length), map_int(author_names, length)),
#     institution = map2(institution, max_length, ~ pad_vector(., .y)),
#     author_names = map2(author_names, max_length, ~ pad_vector(., .y))
#   ) %>%
#   select(id, institution, author_names) %>%
#   unnest(c(institution, author_names)) %>%
#   # Looking at the data at this point, there's a lot of white space around the institutions and author names. Let's remember that now:
#   mutate(institution = str_trim(institution, side = "both"),
#          author_names = str_trim(author_names, side = "both")) %>%
#   full_join(data_id, by = "id") %>%
#   # Whilst we're doing this, we'll also create a counter/number for each institution per paper
#   group_by(id) %>%
#   mutate(author_num = row_number()) %>%
#   ungroup()
#   # We can also pivot to wide format if that makes sense at any point:
#   # pivot_wider(names_from = author_num,
#               # values_from = institution,
#               # names_prefix = "author_",
#               # values_fill = NA_character_)
#
# View(data_locations) # Looks good!
#
# # Uncomment from below this line
#
# # Fortunately, the "maps"  package contains a list of city names that we can use to match with our author institutions. Let's load the relevant data:
# ## Loading country data from package maps
# data(world.cities)
#
#  # The way the matching process works below is by picking the first match in the string, so removing all of the cities below actually leads to an increase in proper matches as the wrong matches are skipped over:
# world_cities_filtered <-  world.cities %>%
#   filter(!name %in% c("China", # There is a city in Mexico called China, and including this in the dataset needed to pick up any papers published in China and link them to this city!
#                         "India", # Same sort of issue (City in Africa)
#                         "San",  # Same sort of issue (City in Africa, again)
#                         "Institut", # This appears to be a City somewhere around Azerbaijan, but I think it's getting picked up as a city when in fact it just is a string in the author address referring to a university!
#                        "Santa", # This is picked up as a city In Peru, when in fact  with just part of the name of many different institutions
#                       "God", # This is picked up as the name of a city in the Hungary, when in fact is just  part of the name of a hospital in Ireland
#                       "Normal", # This is picked up as a city in the US  when in fact it's part of a university name in China
#                       "Bar", # This is picked up as a city in Ukraine, when in fact it's just the name of the University in Israel
#                       "Victoria", #  This leads to confusion between Victoria in Canada and the state in Australia. Easier just to remove rather than be inaccurate
#                       "Cardinal",
#                       "Villanueva", # This is actually a university in Spain, but it's picked up as a city in Honduras
#
#                       "Beira", # This is picked up as a city of Mozambique, but eventually the University Hospital name in Portugal
#                       "Cardinal",
#                       "Young", # This is picked up as a city in Uruguay, when is just part of the name of a young adult hospital in France
#
#                       "Cornwall", # Get picked up as a city in Canada and not the area of the UK
#                        "George", #  A street name in the US,  confused for the South African city
#                        "Imperial", #  part of a UK university name but picked up as a city in Peru
#                       "Laval", # Part of a university name in Canada but gets picked up as the city in France
#                       "Villa", # Part of the name of a institution in Italy, but picked up as the Estonian city.
#                       "Aidu", # Part of the name of a Japanese hospital, but picked up as a Estonian city
#                      "Carolina", # Part of the US state name in the data, but picked up as a city in Peurto Rico
#                        "Carmel", # Get confused with the US city, but always Israel in the dataset
#                               "U",#  seems like shorthand for an address in France, but has picked up as a city in Micronesia and the French city is missed
#                               "Ramon",  # Part of the name of a hospital in Spain, but picked up as the city in the Philippines
#                              "Fundacion", #  Part of the name of a institution in Spain, but picked up as a city in Colombia
#                              "Trinidad", # Part of a institution name in Argentina, were picked up as a city in Bolivia
#                              "Liege",# Picked up as a city in Belgium, but it's actually part of a name of a place in France
#                              "East London", # Refers to the UK University, picked up as a city in South Africa
#                              "Florida", # University name picked up as a city in Cuba
#                              "Ita", # Part of the name of a institution in Finland picked up as the city in Paraguay
#                      "Princeton", #  University name confused for the city in Canada
#                      "Humboldt", # Confused for the Canadian city, but actually a University in Germany
#                       "Alcala", # Confused for the Colombian city, but actually part of a university name in Spain)
#                             "York", # UK Canada confusion. Easier to remove
#                      "Union", # Typically refers to the European Union, but confuse for the US city
#                       "La Rioja", # Part of a Spanish University but computer the city in Argentina
#                      "Concord", # Area in Australia that ends up being linked to a US city rather
#                       "Nanyang", # Part of a Singapore university name that gets linked to a city in China
#                       "Patan", # Part of a Napoleon University name that gets linked to a cityin India
#                       "Saint-Joseph", # University name in Lebanon on that gets linked to a city in Reunion
#                      "Valencia", # University in Spain that gets linked to Venezuela
#                       "Ingenio", # Institution in Spain that gets linked to the Canary Islands
#                      "Lincoln", # UK university name that gets linked to the US
#                      "Roma", # Italian street name that gets linked to Australia
#                      "Leon", # Institution name in France forgets links to Mexico
#                      "Pau", # Part of a hospital name in Spain that Gets linked to France
#                      "Ilan", # Part of a university name in Israel that gets linked to Taiwan
#                      "Street", #  an obvious issue. Linked to the UK incorrectly
#                      "Alle",  # Incorrectly linked to Switzerland when it's an address in Denmark'Hashtag
#                      "San Ignacio", # Location improved its link to Bolivia
#                      "Carnot", # Street in France that gets linked to central Africa
#                       "Mexico", #  country gets incorrectly linked to the Philippines city
#                       "Mobile", #  Institution in Canada that gets incorrectly linked to the US
#                       "Hebron", # Location in Spain gets mixed up with Palestine
#                      "Liban", #  hospital in France that gets linked to Czech Republic
#
#                      "Bayonne", # Addiction clinic in France linked to the US incorrectly
#                      "Apartado", # Confusion between Spain and Colombia. Better to just remove
#                      "Rioja", # Appears in a few different places and can be linked to Spain or Peru
#                       "Li", #  part of a university name in China can be linked to Norway incorrectly
#                      "Al", # Part of the name of places in Saudi Arabia and other countries that gets picked up as a city in Norway
#                      "San Agustin", # Part of the name of a place in Peru that getting linked to Mexico
#                      "Asia", # A city name in the Philippines that is obviously going to give problems
#                      "Jordan", #  country name acts incorrectly linked to the Philippines city
#                      "Kota", # Location in Malaysia that gets linked to India incorrectly
#                      "Ribera", # Part of an institution name in Spain gets linked to Italy
#                      "Pilar", # Name of a Institute in Croatia that gets linked to Brazil incorrectly
#                       "Greenwich", # Causes various problems due to being linked to the US and UK
#                       "George Town", #In Malaysia, baguettes linked to the Cayman Islands
#                      "Worth", # Should the Fort Worth in America, baguettes link to Germany
#                      "Santa Lucia", # Name of a institution in Italy that gets confused at the Canary Islands
#                      "Sainte Anne", # Name of an institution in France that gets confused with the city in Canada
#                      "Douglas", # Part of a university name inCanada gets confused with the Isle of Man
#                      "Arizona", # State name gets confused for a city in Honduras
#                      "Potsdam", # In New York gets confused with Germany
#                      "Kita", # In the name of an Indonesian institution that gets confused with the city in Mali
#                      "Concordia", # US university name gets confused with a town in Argentina
#                      "Bay", # Monterey Bay gets confused with a town in the Philippines
#                      "Parana", # Part of an institution name in Brazil gets linked to Argentina
#                      "Gazi", # Incorrectly gets linked to Ken year when it should be part of a name of a university in Turkey
#                      "Wufeng",# incorrectly linked to China when it should be in Taiwan
#                      "Loo", # Getting correctly linked to Estonia when it's actually part of a institution name in Singapore
#                      "Police", # Police college accidentally linked to city in Poland
#                      "Long", # Thia city and in the name of yale uni address
#                      "David", # City in Panama that causes obvious issues
#                      "Naval", # City in the Philippines that causes obvious issues
#                      "Hall", # City in the Philippines that causes obvious issues
#                      "Trinity" # Irish uni name but gets mistaken to Jersey city
#                      )) %>%
#   # A combination of city names and country names can be used to keep the city where it seems like it can be saved:
#     mutate(city_country = paste0(name,", ", country.etc)) %>%
#   filter(!city_country %in% c("Sussex, Canada",# Only the UK one appears and this gets confused
#                               "Milton, Canada", #  should be Milton Keynes in the UK
#                               "Bathurst, Canada", # Location in Australia they get confused Canada
#                               "Milton, New Zealand", #  should be Milton Keynes in the UK
#                               "Orleans, France", #  incorrectly linked to France, not Canada
#                               "Bergen, Norway", # teams to be linked consistently to the US,  but mistaken for Norway
#                               "Penrith, UK", # Should be the Australian city near Sydney
#                               "Bedford, UK", #  location in Australia gets Linked to the UK incorrectly
#                               "Bedford, USA", #  location in Australia gets Linked to the US incorrectly
#                               "Salt, Spain", # Should be Salt Lake City, US,  but gets picked up as the Spanish city
#                               "Lancaster, USA", #  should be the UK
#                               "Ho, Ghana", # Part of an address in Taiwan that's getting linked to Ghana
#                               "Laguna, USA", # Should be the canary islands, surprisingly
#                               "Albert, France",
#                               "Hong, Denmark",# Leads to this being picked up and said of Hong Kong
#                              "Durham, USA", # Should be the UK one
#                              "Brest, Belarus", # Refers to France not Belarus
#                              "Warwick, USA", #  Always seems to refer to the UK university, but the Use of the US city
#                              "Belmont, Canada", #  Always seems to refer to the US,  but confused for the Canadian city
#                              "Beaufort, Malaysia", # Should be the American city
#                              "Mackay, Australia", #  should always be the location in Taiwan
#                              "Alicante, Philippines", #  should always be in Spain
#                              "Malaya, Philippines", # Should be in Malaysia
#                            "Claremont, Jamaica", #  Australian location that gets sent to Jamaica, incorrectly
#
#                            "Colombia, Cuba", # Get confused with the country
#                            "Carlton, UK", #Street name in Canada gets confused with the UK
#                            "Costa Rica, Mexico", # Obviously get complete. The country
#                            "Notre Dame, Mauritius", #  actually the Australian University!
#                             "Baja, Hungary", # University namely Mexico
#                            "Palmerston, Australia", #  links to the northern Australian city, rather than New Zealand
#                            "Waterloo, USA" # Always the Canadian university
#          )) %>%
#    group_by(name) %>%
#   filter(pop == max(pop)) %>% # Okay, so this is imperfect, but when a city name is duplicated that I haven't accounted for above, this will filter to select only the one with the highest population. This is based on the assumption that papers are likely to come from more populated cities (i.e. those with universities). This may seem crude, but it solved many, many issues in the map.
#   ungroup()
#
#
# # Extract just the city names so we can try and match author locations using these:
# city_names_from_world <- world_cities_filtered$name
#
# # Create a pattern of city names for matching with word boundaries:
# city_names_pattern <- paste("\\b(", paste(city_names_from_world, collapse = "|"), ")\\b", sep = "")
#
#
#
# # Extract city names using stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match we're looking for)
# cities_of_authors <- str_extract(data_locations$institution, city_names_pattern)
#
#
#
# # Before we join this to our dataset, let's now do the same thing for countries, using the world dataset. We could just link countries to the existing cities we identified, but this won't give us the richest overall data, as in some cases it might find a country name but no city name and vice versa.
#
# # Extract just the Country names so we can try and match author locations using these:
# country_names <- world.cities$country.etc
#
# # Create a pattern of Country names for matching with word boundaries:
# country_names_pattern <- paste("\\b(", paste(country_names, collapse = "|"), ")\\b", sep = "")
#
# # Extract country names using stringr (the str_exact function extracts the first complete match from a string; the arguments are the string and then the match we're looking for)
# countries_of_authors <- str_extract(data_locations$institution, country_names_pattern)
#
#
#
# # Add the extracted city and country names to our dataset:
# data_locations_with_city_country<- data_locations %>%
#   bind_cols(cities_of_authors,
#             countries_of_authors) %>%
#   rename(cities_of_authors = 19,
#          countries_of_authors = 20)
# # Check everything looks okay:
# #  select(author_names,
# #         cities_of_authors,
# #         countries_of_authors) %>%
# #  print(n=150)
#
# # Make the city name column consistent with paper dataset:
# world_cities_filtered <- as_tibble(world_cities_filtered) %>%
#   rename(cities_of_authors = name)
#
# #Join the world.cities dataset with our paper data so we have latitude and longitude for each city:
# data_locations_with_full_geo_location<-
#   left_join(data_locations_with_city_country,
#            world_cities_filtered,
#            by = join_by(cities_of_authors) # This is done purposely, as I want to check whether the country names matched in text above match with the country names linked to the city names. Any mismatches tell us a lot about whether it got the right city are not and how I filtered out most of the problematic cities above!
#            )
#
# # Now, Save this to a CSV file because this took forever to extract the data we don't want to have to do this every time I render this page!!
write.csv(Data temps/data_locations_with_full_geo_location, "data_locations_with_full_geo_location.csv")
data %>%
count(Language) %>%
arrange(desc(n)) %>%
gt()
language_map <- c(
"eng" = "English", "ger" = "German", "fre" = "French", "rus" = "Russian",
"spa" = "Spanish", "hun" = "Hungarian", "jpn" = "Japanese", "chi" = "Chinese",
"pol" = "Polish", "ita" = "Italian", "kor" = "Korean", "cze" = "Czech",
"por" = "Portuguese", "heb" = "Hebrew", "dut" = "Dutch", "gre" = "Greek",
"swe" = "Swedish", "fin" = "Finnish", "nor" = "Norwegian", "dan" = "Danish",
"srp" = "Serbian", "tur" = "Turkish", "hrv" = "Croatian", "lit" = "Lithuanian",
"eng; spa" = "English; Spanish", "spa; eng" = "English; Spanish",
"por; eng" = "English; Portuguese", "eng; por" = "English; Portuguese",
"eng; jpn" = "English; Japanese", "jpn; eng" = "English; Japanese",
"eng; pol" = "English; Polish", "pol; eng" = "English; Polish",
"eng; tur" = "English; Turkish", "tur; eng" = "English; Turkish",
"eng; chi" = "English; Chinese",
"eng; fre" = "English; French",
"eng; ger" = "English; German",
"por; spa; eng"	= "Portuguese; Spanish"
)
data_w_languages <- data %>%
mutate(Language_recoded = language_map[Language])
data_w_languages %>%
distinct(PMID, .keep_all = TRUE) %>%
count(Language,
Language_recoded) %>%
arrange(desc(n)) %>%
gt()
