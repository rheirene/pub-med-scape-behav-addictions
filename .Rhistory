relocate(Title) %>% # Place full title first for ease
as_tibble() %>%
print()
# Check results:
# View(results_behav_addictions2)
# Check dupliactes based on PMIDs:
results_behav_addictions2 %>%
group_by(PMID) %>%
filter(n()>1) %>%
print(n = 100)
# There is no evidence of clear PMID duplicates
# Show duplicate titles:
results_behav_addictions2 %>%
group_by(Title) %>%
filter(n()>1)
# **This is actually two seperate commentaries on the same article**
#  Okay, now we are confident that we have a clean dataset, let's explore!
# Summary of the Year  variable:
summary(results_behav_addictions2$Year)
# Number of publications per year:
hist(results_behav_addictions2$Year,
xlim = c(1960,2023),
breaks = 30,
main = "Papers published per year on 'behavioural addictions' (PubMed)",
xlab = "Year") # Even though the earliest year an article was published is 1994, I've set the range to started 1960 as this is where the starting point is across all other behavioural addiction searches.
# Number of separate journals:
results_behav_addictions2 %>%
distinct(Journal_name_short) %>%
arrange(Journal_name_short) %>%
print(n = 50)
# Most popular journals:
results_behav_addictions2 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 50)
# View(results_behav_addictions_sans_duplicates) # Visually inspect results
# Now I need to add a label to all of these studies to signify that they Were returned from the behav_addictions search so that we can distinguish them from other studies when we later joined the datasets together:
Label <- rep("behavioural_addictions", times = count(results_behav_addictions2))
results_behav_addictions_final <- results_behav_addictions2 %>% bind_cols(Label) %>%
rename(Label = 14) %>%
print()
# View(results_behav_addictions_final)
# Now save the cleaned results:
write.csv(results_behav_addictions_final, "Data extraction/behav_addictions_data_cleaned.csv", row.names=FALSE)
#  Clean environment:
rm(list = ls())
# Add search URL:
url_shopping_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%22buying-shopping+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22shopping+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22buying+addiction%22%5BTitle%2FAbstract%5D%29&size=200" # 73 results returned on 20/07/2023
# Read and parse the webpage
webpage_shopping_rvest <- read_html(url_shopping_rvest)
#| code-fold: true
#| code-summary: "Set-up code"
# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install
library(groundhog) # Load
# List desired packages:
packages <- c('rentrez',
'dplyr',
'tidyr',
'purrr',
'rvest',
'stringr')
# Load desired package with versions specific to project start date:
groundhog.library(packages, "2023-06-07")
# Add search URL:
url_shopping_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%22buying-shopping+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22shopping+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22buying+addiction%22%5BTitle%2FAbstract%5D%29&size=200" # 73 results returned on 20/07/2023
# Read and parse the webpage
webpage_shopping_rvest <- read_html(url_shopping_rvest)
# Get the total number of search results
results_count_shopping <- webpage_shopping_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages
results_per_page_shopping <- 50
total_pages_shopping <- ceiling(results_count_shopping[1] / results_per_page_shopping)
# Print results_count_shopping and total_pages_shopping
print(results_count_shopping)
print(total_pages_shopping)
# Initialize an empty data frame
results_shopping_rvest <- data.frame()
# Loop through each page and scrape the data
for (page in 0:(total_pages_shopping - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_shopping_rvest, "&page=", page + 1)
# Read and parse the current webpage
current_page <- read_html(current_url)
# Extract title
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title info on page", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data
current_results_shopping <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results
results_shopping_rvest <- rbind(results_shopping_rvest, current_results_shopping)
# Introduce a delay to avoid overloading the server
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
# Print the extracted data
print(as_tibble(results_shopping_rvest))
# Now save the titles before proceeding so we can't lose them!
write.csv(results_shopping_rvest, "Data extraction/shopping_titles.csv", row.names=FALSE)
results_shopping<- read.csv("Data extraction/shopping_data.csv") %>%
as_tibble()
# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_shopping_rvest<- read.csv("Data extraction/shopping_titles.csv") %>%
as_tibble()
# Merge our two datasets
results_shopping2 <- results_shopping %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_shopping_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_shopping2 %>%
distinct(Title) %>%
count() %>%
print() # n = 1 dupes (jUly 2023)
# Show duplicate:
results_shopping2 %>%
group_by(Title) %>%
filter(n()>1) %>%
# View()
print() # In July 2023  this returns to studies with the same title (Technological Addictions). Despite having the same title, authors, and been published at the same time of year in 2022, these are actually separate papers... (Determined by manually searching the articles)
# Have a look at duplicates where the title, year, and authors are all the same:
results_shopping2 %>%
group_by(Title,
Journal_name_short,
Full_Author_Name) %>%
filter(n()>1)
# Have a look at duplicates were just the title, journal and year are the same:
results_shopping2 %>%
group_by(Title,
Journal_name_short,
Year) %>%
filter(n()>1) %>%
print() # None
# Summary of the Year  variable:
summary(results_shopping2$Year)
# Number of publications per year:
shopping_hist<- hist(results_shopping2$Year,
xlim = c(1960,2023),
breaks = 10,
main = "Papers published per year on shopping Disorder (PubMed)",
xlab = "Year")
# Number of separate journals:
results_shopping2 %>%
distinct(Journal_name_short) %>%
arrange(Journal_name_short) %>%
print(n = 20)
# Most popular journals:
results_shopping2 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
# View(results_shopping2) # Visually inspect results
# Now I need to add a label to all of these studies to signify that they Were returned from the shopping search so that we can distinguish them from other studies when we later joined the datasets together:
Label <- rep("shopping", times = count(results_shopping2))
results_shopping_final <- results_shopping2 %>% bind_cols(Label) %>%
rename(Label = 14) %>%
print()
# View(results_shopping_final)
# Now save the cleaned results
write.csv(results_shopping_final, "Data extraction/shopping_data_cleaned.csv", row.names=FALSE)
# Number of publications per year:
shopping_hist<- hist(results_shopping2$Year,
xlim = c(1960,2023),
breaks = 10,
main = "Papers published per year on Shopping Addiction (PubMed)",
xlab = "Year")
#  Clean environment:
rm(list = ls())
#  Clean environment:
rm(list = ls())
# Define the search term:
search_term_social_media <- '((("internet addiction"[Title/Abstract]) OR ("addictive internet use"[Title/Abstract])) OR ("internet use disorder"[Title/Abstract])) OR ("addiction to the internet"[Title/Abstract])'
# Use entrez_search to get the IDs of the articles:
search_results_social_media <- entrez_search(db="pubmed", term=search_term_social_media, retmax=20000)
#| code-fold: true
#| code-summary: "Set-up code"
# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install
library(groundhog) # Load
# List desired packages:
packages <- c('rentrez',
'dplyr',
'tidyr',
'purrr',
'rvest',
'stringr')
# Load desired package with versions specific to project start date:
groundhog.library(packages, "2023-06-07")
#  Clean environment:
rm(list = ls())
# Define the search term:
search_term_social_media <- '((("internet addiction"[Title/Abstract]) OR ("addictive internet use"[Title/Abstract])) OR ("internet use disorder"[Title/Abstract])) OR ("addiction to the internet"[Title/Abstract])'
# Use entrez_search to get the IDs of the articles:
search_results_social_media <- entrez_search(db="pubmed", term=search_term_social_media, retmax=20000)
id_list_social_media <- search_results_social_media$ids
# Split id list into chunks of 500:
chunks <- split(id_list_social_media, ceiling(seq_along(id_list_social_media)/100))
# Fetch details for each chunk of articles:
article_details_social_media <- map_dfr(chunks, function(ids) {
# Fetch details of the articles:
articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
# Split the articles into individual articles:
articles <- strsplit(articles, "\n\n")[[1]]
# Process each article:
map_dfr(articles, function(article) {
# Split the article into lines:
lines <- strsplit(article, "\n")[[1]]
# Get the details we're interested in:
details_social_media <- list(
PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_social_media <- bind_rows(details_social_media)
return(details_df_social_media)
})
})
# Check the results:
article_details_social_media %>%
as_tibble() %>%
print()
# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
'COIS- ' = '', 'JT  - ' = '')
results_social_media <- article_details_social_media %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename(# Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_social_media, "Data extraction/social_media_data.csv", row.names=FALSE)
# Add search URL:
url_social_media_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=(((%22internet%20addiction%22%5BTitle%2FAbstract%5D)%20OR%20(%22addictive%20internet%20use%22%5BTitle%2FAbstract%5D))%20OR%20(%22internet%20use%20disorder%22%5BTitle%2FAbstract%5D))%20OR%20(%22addiction%20to%20the%20internet%22%5BTitle%2FAbstract%5D)&size=200" # 2335 results returned on 20/07/2023
# Read and parse the webpage
webpage_social_media_rvest <- read_html(url_social_media_rvest)
# Get the total number of search results
results_count_social_media <- webpage_social_media_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages
results_per_page_social_media <- 50
total_pages_social_media <- ceiling(results_count_social_media[1] / results_per_page_social_media)
# Print results_count_social_media and total_pages_social_media
print(results_count_social_media)
print(total_pages_social_media)
# Initialize an empty data frame
results_social_media_rvest <- data.frame()
# Loop through each page and scrape the data
for (page in 0:(total_pages_social_media - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_social_media_rvest, "&page=", page + 1)
# Read and parse the current webpage
current_page <- read_html(current_url)
# Extract title
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title info on page", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data
current_results_social_media <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results
results_social_media_rvest <- rbind(results_social_media_rvest, current_results_social_media)
# Introduce a delay to avoid overloading the server
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
# Add search URL:
url_internet_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=(((%22internet%20addiction%22%5BTitle%2FAbstract%5D)%20OR%20(%22addictive%20internet%20use%22%5BTitle%2FAbstract%5D))%20OR%20(%22internet%20use%20disorder%22%5BTitle%2FAbstract%5D))%20OR%20(%22addiction%20to%20the%20internet%22%5BTitle%2FAbstract%5D)&size=200" # 2335 results returned on 20/07/2023
# Read and parse the webpage
webpage_internet_rvest <- read_html(url_internet_rvest)
# Add search URL:
url_internet_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=(((%22internet%20addiction%22%5BTitle%2FAbstract%5D)%20OR%20(%22addictive%20internet%20use%22%5BTitle%2FAbstract%5D))%20OR%20(%22internet%20use%20disorder%22%5BTitle%2FAbstract%5D))%20OR%20(%22addiction%20to%20the%20internet%22%5BTitle%2FAbstract%5D)&size=200" # 2335 results returned on 20/07/2023
# Read and parse the webpage
webpage_internet_rvest <- read_html(url_internet_rvest)
# Get the total number of search results
results_count_internet <- webpage_internet_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages
results_per_page_internet <- 50
total_pages_internet <- ceiling(results_count_internet[1] / results_per_page_internet)
# Print results_count_internet and total_pages_internet
print(results_count_internet)
print(total_pages_internet)
# Initialize an empty data frame
results_internet_rvest <- data.frame()
# Loop through each page and scrape the data
for (page in 0:(total_pages_internet - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_internet_rvest, "&page=", page + 1)
# Read and parse the current webpage
current_page <- read_html(current_url)
# Extract title
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title info on page", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data
current_results_internet <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results
results_internet_rvest <- rbind(results_internet_rvest, current_results_internet)
# Introduce a delay to avoid overloading the server
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
#  Clean environment:
rm(list = ls())
# Define the search term:
search_term_social_media <- '(((("social_media addiction"[Title/Abstract]) OR ("social_media dependence"[Title/Abstract])) OR ("compulsory social_media"[Title/Abstract])) OR ("obligatory social_media"[Title/Abstract])) OR ("Addiction to social_media"[Title/Abstract])'
# Use entrez_search to get the IDs of the articles:
search_results_social_media <- entrez_search(db="pubmed", term=search_term_social_media, retmax=20000)
id_list_social_media <- search_results_social_media$ids
# Split id list into chunks of 500:
chunks <- split(id_list_social_media, ceiling(seq_along(id_list_social_media)/100))
# Fetch details for each chunk of articles:
article_details_social_media <- map_dfr(chunks, function(ids) {
# Fetch details of the articles:
articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
# Split the articles into individual articles:
articles <- strsplit(articles, "\n\n")[[1]]
# Process each article:
map_dfr(articles, function(article) {
# Split the article into lines:
lines <- strsplit(article, "\n")[[1]]
# Get the details we're interested in:
details_social_media <- list(
PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_social_media <- bind_rows(details_social_media)
return(details_df_social_media)
})
})
# Check the results:
article_details_social_media %>%
as_tibble() %>%
print()
# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
'COIS- ' = '', 'JT  - ' = '')
results_social_media <- article_details_social_media %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename(# Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_social_media, "Data extraction/social_media_data.csv", row.names=FALSE)
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_social_media<- read.csv("Data extraction/social_media_data.csv") %>%
as_tibble()
# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_social_media_rvest<- read.csv("Data extraction/social_media_titles.csv") %>%
as_tibble()
# Merge our two datasets
results_social_media2 <- results_social_media %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_social_media_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Check results:
# View(results_social_media2)
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_social_media2 %>%
distinct(Title) %>%
count() %>%
print() # n = 1 dupes (jUly 2023)
# Show duplicate:
results_social_media2 %>%
group_by(Title) %>%
filter(n()>1) %>%
# View()
print() # In July 2023  this returns to studies with the same title (Technological Addictions). Despite having the same title, authors, and been published at the same time of year in 2022, these are actually separate papers... (Determined by manually searching the articles)
# Have a look at duplicates where the title, year, and authors are all the same:
results_social_media2 %>%
group_by(Title,
Journal_name_short,
Full_Author_Name) %>%
filter(n()>1)
# print() # None
# Have a look at duplicates were just the title, journal and year are the same:
results_social_media2 %>%
group_by(Title,
Journal_name_short,
Year) %>%
filter(n()>1) %>%
print() # None
#  Okay, now we're confident we have a clean dataset, let's explore!
# Summary of the Year  variable:
summary(results_social_media2$Year)
# Number of publications per year:
social_media_hist<- hist(results_social_media2$Year,
xlim = c(1960,2023),
breaks = 10,
main = "Papers published per year on Social Media Addiction (PubMed)",
xlab = "Year")
# Number of separate journals:
results_social_media2 %>%
distinct(Journal_name_short) %>%
arrange(Journal_name_short) %>%
print(n = 20)
# Most popular journals:
results_social_media2 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
# View(results_social_media2) # Visually inspect results
# Now I need to add a label to all of these studies to signify that they Were returned from the social_media search so that we can distinguish them from other studies when we later joined the datasets together:
Label <- rep("social_media", times = count(results_social_media2))
results_social_media_final <- results_social_media2 %>% bind_cols(Label) %>%
rename(Label = 14) %>%
print()
# View(results_social_media_final)
# Now save the cleaned results
write.csv(results_social_media_final, "Data extraction/social_media_data_cleaned.csv", row.names=FALSE)
