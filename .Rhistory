id_list_gambling <- search_results_gambling$ids
# Split id list into chunks of 500:
chunks <- split(id_list_gambling, ceiling(seq_along(id_list_gambling)/100))
# Fetch details for each chunk of articles:
article_details_gambling <- map_dfr(chunks, function(ids) {
# Fetch details of the articles:
articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
# Split the articles into individual articles:
articles <- strsplit(articles, "\n\n")[[1]]
# Process each article:
map_dfr(articles, function(article) {
# Split the article into lines:
lines <- strsplit(article, "\n")[[1]]
# Get the details we're interested in:
details_gambling <- list(
PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_gambling <- bind_rows(details_gambling)
return(details_df_gambling)
})
})
# Check the results:
article_details_gambling %>%
as_tibble() %>%
print()
# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
'COIS- ' = '', 'JT  - ' = '')
results_gambling <- article_details_gambling %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename( # Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
library(tidyr)
results_gambling <- article_details_gambling %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename( # Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
library(stringr)
results_gambling <- article_details_gambling %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename( # Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_gambling, "ALL_gambling_data.csv", row.names=FALSE)
results_gambling %>%
group_by(Year) %>%
count()
results_gambling %>%
group_by(Year) %>%
count() %>%
view()
results_gambling %>%
group_by(Year) %>%
count() %>%
view()
results_gambling %>%
group_by(Year) %>%
count() %>%
View()
results_gambling %>%
group_by(Year) %>%
count() %>%
write.csv("ALL_gambling_data_BY_YEAR_N.csv", row.names=FALSE)
results_gambling %>%
group_by(Year) %>%
count() %>%
sum()
results_gambling %>%
group_by(Year) %>%
sum()
results_gambling %>%
group_by(Year) %>%
sum(n)
results_gambling %>%
group_by(Year) %>%
sum(Year)
results_gambling
results_gambling %>%
n()
results_gambling %>%
count()
results_gambling %>%
group_by(Year) %>%
flter(Year < 2011) %>%
count()
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011) %>%
count()
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011) %>%
summarise(
n = sum(n)
)
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011) %>%
summarise(
sum(n)
)
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011) %>%
summarize(
sum(n)
)
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011)
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011) %>%
sum(n)
results_gambling %>%
group_by(Year) %>%
filter(Year < 2011)
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year < 2011) %>%
sum(n)
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year < 2011)
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year < 2011) %>%
summarize(total= sum(n))
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year < 2011) %>%
summarize(total= sum(n))
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year < 2011) %>%
ungroup() %>%
summarize(total= sum(n))
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year => 2011) %>%
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year => 2011) %>%
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year >= 2011) %>%
ungroup() %>%
summarize(total= sum(n)) #
results_gambling %>%
count()
2731/count(results_gambling)*100
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year < 2013) %>%
ungroup() %>%
summarize(total= sum(n)) # 2731
results_gambling %>%
group_by(Year) %>%
count() %>%
filter(Year >= 2013) %>%
ungroup() %>%
summarize(total= sum(n)) # 6684
3496/count(results_gambling)*100
5919/count(results_gambling)*100
#| code-fold: true
#| code-summary: "Set-up code"
# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install
library(groundhog) # Load
# List desired packages:
packages <- c(rentrez,
dplyr,
tidyr,
purrr,
rvest,
stringr)
#| code-fold: true
#| code-summary: "Set-up code"
# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install
library(groundhog) # Load
# List desired packages:
packages <- c('rentrez',
'dplyr',
'tidyr',
'purrr',
'rvest',
'stringr')
# Load desired package with versions specific to project start date:
groundhog.library(packages, "2023-06-07")
#  Clean environment:
rm(list = ls())
# Define the search term:
search_term_gambling <- '("gambling disorder"[Title/Abstract]) OR ("disordered gambling"[Title/Abstract] OR "gambling addiction"[Title/Abstract]) OR ("addicted gambler*"[Title/Abstract]) OR ("pathological gambl*"[Title/Abstract]) OR ("compulsive gambl*"[Title/Abstract])'
# Use entrez_search to get the IDs of the articles:
search_results_gambling <- entrez_search(db="pubmed", term=search_term_gambling, retmax=20000)
id_list_gambling <- search_results_gambling$ids
# Split id list into chunks of 500:
chunks <- split(id_list_gambling, ceiling(seq_along(id_list_gambling)/100))
# Fetch details for each chunk of articles:
article_details_gambling <- map_dfr(chunks, function(ids) {
articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
# Split the articles into individual articles:
articles <- strsplit(articles, "\n\n")[[1]]
# Process each article:
map_dfr(articles, function(article) {
# Split the article into lines:
lines <- strsplit(article, "\n")[[1]]
# Get the details we're interested in:
details_gambling <- list(
PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_gambling <- bind_rows(details_gambling)
return(details_df_gambling)
})
})
#  Clean environment:
rm(list = ls())
# Define the search term:
search_term_gambling <- '("gambling disorder"[Title/Abstract]) OR ("disordered gambling"[Title/Abstract] OR "gambling addiction"[Title/Abstract]) OR ("addicted gambler*"[Title/Abstract]) OR ("pathological gambl*"[Title/Abstract]) OR ("compulsive gambl*"[Title/Abstract])'
# Use entrez_search to get the IDs of the articles:
search_results_gambling <- entrez_search(db="pubmed", term=search_term_gambling, retmax=20000)
id_list_gambling <- search_results_gambling$ids
# Split id list into chunks of 500:
chunks <- split(id_list_gambling, ceiling(seq_along(id_list_gambling)/100))
# Fetch details for each chunk of articles:
article_details_gambling <- map_dfr(chunks, function(ids) {
articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
# Split the articles into individual articles:
articles <- strsplit(articles, "\n\n")[[1]]
# Process each article:
map_dfr(articles, function(article) {
# Split the article into lines:
lines <- strsplit(article, "\n")[[1]]
# Get the details we're interested in:
details_gambling <- list(
PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_gambling <- bind_rows(details_gambling)
return(details_df_gambling)
})
})
# Check the results:
article_details_gambling %>%
as_tibble() %>%
print()
# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
'COIS- ' = '', 'JT  - ' = '')
results_gambling <- article_details_gambling %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename( # Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_gambling, "Data extraction/gambling_data.csv", row.names=FALSE)
# Add search URL
url_gambling_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%22gambling+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22disordered+gambling%22%5BTitle%2FAbstract%5D+OR+%22gambling+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addicted+gambler*%22%5BTitle%2FAbstract%5D%29+OR+%28%22pathological+gambl*%22%5BTitle%2FAbstract%5D%29+OR+%28%22compulsive+gambl*%22%5BTitle%2FAbstract%5D%29&size=200"
# Read and parse the webpage
webpage_gambling_rvest <- read_html(url_gambling_rvest)
# Get the total number of search results
results_count_gambling <- webpage_gambling_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages
results_per_page_gambling <- 100
total_pages_gambling <- ceiling(results_count_gambling[1] / results_per_page_gambling)
# Print results_count_gambling and total_pages_gambling
print(results_count_gambling)
print(total_pages_gambling)
# Initialize an empty data frame
results_gambling_rvest <- data.frame()
# Loop through each page and scrape the data
for (page in 0:(total_pages_gambling - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_gambling_rvest, "&page=", page + 1)
# Read and parse the current webpage
current_page <- read_html(current_url)
# Extract title
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title info on page", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data
current_results_gambling <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results
results_gambling_rvest <- rbind(results_gambling_rvest, current_results_gambling)
# Introduce a delay to avoid overloading the server
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
# Print the extracted data
print(as_tibble(results_gambling_rvest))
# Print the extracted data
print(as_tibble(results_gambling_rvest))
# Merge the two datasets
results_gambling2 <- results_gambling %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from the primary dataset
merge(results_gambling_rvest, by = "PMID") %>%
relocate(Title) %>% # Place full title first for ease
as_tibble() %>%
print()
# Check dupliactes based on PMIDs:
Simple_duplicate_removal_n <- results_gambling2 %>%
distinct(PMID) %>%
count() %>%
print()
results_gambling2 %>%
group_by(PMID) %>%
filter(n()>1) %>%
print(n = 100)
results_gambling2 %>%
group_by(PMID) %>%
filter(n()>1) %>%
print(n = 100)
# Check dupliactes based on PMIDs:
Simple_duplicate_removal_n <- results_gambling2 %>%
distinct(PMID) %>%
count() %>%
print()
results_gambling2
results_gambling
results_gambling_rvest
results_gambling2 %>%
group_by(PMID) %>%
filter(n()>1) %>%
print(n = 100)
results_gambling2 %>%
group_by(PMID) %>%
filter(n()>1) %>%
print(n = 100)
# Total number with basic duplication removal based on title (there are some distinct papers with the same title like "gambling disorder", so this is too simplified, but it'll do for a quick check):
Simple_duplicate_removal_n <- results_gambling2 %>%
distinct(Title) %>%
count() %>%
print()
# How many duplicates (based on title alone) does this remove?
count(results_gambling_sans_duplicates) - Simple_duplicate_removal_n
# How many duplicates (based on title alone) does this remove?
count(results_gambling_sans_duplicates) - Simple_duplicate_removal_n
# Remove these asap as they're clear duplicates:
results_gambling_sans_duplicates<- results_gambling2 %>%
distinct(PMID, .keep_all = TRUE) %>%
print()
# Total number with basic duplication removal based on title (there are some distinct papers with the same title like "gambling disorder", so this is too simplified, but it'll do for a quick check):
Simple_duplicate_removal_n <- results_gambling2 %>%
distinct(Title) %>%
count() %>%
print()
# How many duplicates (based on title alone) does this remove?
count(results_gambling_sans_duplicates) - Simple_duplicate_removal_n
# Have a look at duplicates where the title, year, and authors are all the same:
results_gambling_sans_duplicates %>%
group_by(Title, Journal_name_short, Full_Author_Name) %>%
filter(n()>1) %>%
print()
# Have a look at duplicates were just the title, journal and year are the same:
results_gambling_sans_duplicates %>%
group_by(Title, Journal_name_short, Year) %>%
filter(n()>1) %>%
View()
# Remove duplicates based on only duplicated titles and authors:
results_gambling_sans_duplicates2<- results_gambling_sans_duplicates %>%
filter(PMID != "31346181") %>%
print()
# Remove duplicates based on above observation:
results_gambling_sans_duplicates2<- results_gambling_sans_duplicates %>%
filter(PMID != "31346181") %>%
print()
# Check this has removed the duplicate:
count(results_gambling_sans_duplicates) - count(results_gambling_sans_duplicates2)
# Summary of the Year  variable:
summary(results_gambling_sans_duplicates2$Year)
# Number of publications per year:
Gambling_hist<- hist(results_gambling_sans_duplicates2$Year,
xlim = c(1960,2023),
breaks = 60,
main = "Papers published per year on Gambling Disorder (PubMed)",
xlab = "Year")
# Number of separate journals:
results_gambling_sans_duplicates2 %>%
distinct(Journal_name_short) %>%
arrange(Journal_name_short) %>%
print(n = 50)
# Most popular journals:
results_gambling_sans_duplicates2 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 50)
# Now I need to add a label to all of these studies to signify that they Were returned from the gambling search so that we can distinguish them from other studies when we later joined the datasets together:
count(results_gambling_sans_duplicates2)
Label <- rep("gambling", times = count(results_gambling_sans_duplicates2))
results_gambling_final <- results_gambling_sans_duplicates2 %>% bind_cols(Label) %>%
rename(Label = 14) %>%
print()
write.csv(results_gambling_final, "C:/Users/rheirene/Dropbox (Personal)/Sci-Software/Blog posts/PubMed Article Scraping/Web scrape scholar/Data extraction/gambling_data_cleaned.csv", row.names=FALSE)
write.csv(results_gambling_final, "Data extraction/gambling_data_cleaned.csv", row.names=FALSE)
results_gambling_sans_duplicates2
results_gambling
read.csv("Data extraction/gambling_data_cleaned.csv")
