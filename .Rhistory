PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_gaming <- bind_rows(details_gaming)
return(details_df_gaming)
})
})
# Check the results:
article_details_gaming %>%
as_tibble() %>%
print()
# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
'COIS- ' = '', 'JT  - ' = '')
results_gaming <- article_details_gaming %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename(# Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_gaming, "Data extraction/gaming_data.csv", row.names=FALSE)
# Add search URL:
url_gaming_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%28%22gaming+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22internet+gaming+disorder%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22gaming+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22video+game+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22video+game+disorder%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22gaming+dependence%22%29&size=200"
# Read and parse the webpage
webpage_gaming_rvest <- read_html(url_gaming_rvest)
# Get the total number of search results
results_count_gaming <- webpage_gaming_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages
results_per_page_gaming <- 50
total_pages_gaming <- ceiling(results_count_gaming[1] / results_per_page_gaming)
# Print results_count_gaming and total_pages_gaming
print(results_count_gaming)
print(total_pages_gaming)
# Initialize an empty data frame
results_gaming_rvest <- data.frame()
# Loop through each page and scrape the data
for (page in 0:(total_pages_gaming - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_gaming_rvest, "&page=", page + 1)
# Read and parse the current webpage
current_page <- read_html(current_url)
# Extract title
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title info on page", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID:
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data:
current_results_gaming <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results:
results_gaming_rvest <- rbind(results_gaming_rvest, current_results_gaming)
# Introduce a delay to avoid overloading the server:
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
# Print the extracted data:
print(as_tibble(results_gaming_rvest))
# Merge our two datasets
results_gaming2 <- results_gaming %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_gaming_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Check results:
# View(results_gaming2)
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_gaming2 %>%
distinct(Title) %>%
count() %>%
print()
# Show duplicate:
results_gaming2 %>%
group_by(Title) %>%
filter(n()>1) %>%
print() # This is actually two seperatre studies
# I know from manual searching that there is a genuine duplicate that is a corrigendum to one study. The PMID  for the corrigendum is 35543161 (Remove later)
# Have a look at duplicates where the title, year, and authors are all the same:
results_gaming2 %>%
group_by(Title,
Journal_name_short,
Full_Author_Name) %>%
filter(n()>1)
print()
# Merge our two datasets
results_gaming2 <- results_gaming %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_gaming_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Merge the two datasets:
results_gambling2 <- results_gambling %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from the primary dataset
merge(results_gambling_rvest, by = "PMID") %>%
relocate(Title) %>% # Place full title first for ease
as_tibble() %>%
print()
# Merge the two datasets:
results_gambling2 <- results_gambling %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from the primary dataset
merge(results_gambling_rvest, by = "PMID") %>%
relocate(Title) %>% # Place full title first for ease
as_tibble() %>%
print()
results_gambling<- read.csv("Data extraction/gambling_data.csv")
# Load in first dataset in case environment clean on knit:
results_gambling<- read.csv("Data extraction/gambling_data.csv")
# Load in first dataset in case environment clean on knit (above section not excuted for render):
results_gaming<- read.csv("Data extraction/gaming_data.csv")
# Merge our two datasets
results_gaming2 <- results_gaming %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_gaming_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Merge our two datasets
results_gaming2 <- results_gaming %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_gaming_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
results_gaming
# Load in first dataset in case environment clean on knit (above section not excuted for render):
results_gaming<- read.csv("Data extraction/gaming_data.csv") %>%
as_tibble()
results_gaming
# Load in first dataset in case environment clean on knit (above section not excuted for render):
results_gambling<- read.csv("Data extraction/gambling_data.csv") %>%
as_tibble()
results_gambling
# Merge the two datasets:
results_gambling2 <- results_gambling %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from the primary dataset
merge(results_gambling_rvest, by = "PMID") %>%
relocate(Title) %>% # Place full title first for ease
as_tibble() %>%
print()
# Now save the titles results before proceeding so we can't lose them!
write.csv(results_gambling_rvest, "Data extraction/gambling_titles.csvv", row.names=FALSE)
# Now save the titles before proceeding so we can't lose them!
write.csv(results_gaming_rvest, "Data extraction/gaming_titles.csv", row.names=FALSE)
# Load in first dataset in case environment clean on knit (above sections not excuted for render):
results_gaming<- read.csv("Data extraction/gaming_data.csv") %>%
as_tibble()
results_gaming
# Load in second dataset in case environment clean on knit (above sections not excuted for render):
results_gaming_rvest<- read.csv("Data extraction/gaming_titles.csv") %>%
as_tibble()
results_gaming_rvest
# Merge our two datasets:
results_gaming2 <- results_gaming %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_gaming_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_gaming2 %>%
distinct(Title) %>%
count() %>%
print()
# Show duplicate:
results_gaming2 %>%
group_by(Title) %>%
filter(n()>1) %>%
print() # This is actually two seperatre studies
# Have a look at duplicates where the title, year, and authors are all the same:
results_gaming2 %>%
group_by(Title,
Journal_name_short,
Full_Author_Name) %>%
filter(n()>1)
# Have a look at duplicates were just the title, journal and year are the same:
results_gaming2 %>%
group_by(Title,
Journal_name_short,
Year) %>%
filter(n()>1) %>%
print()
# Remove the duplicate study:
results_gaming_sans_duplicates<- results_gaming2 %>%
filter(PMID != "35543161") %>%
print()
# Show duplicate:
results_gaming2 %>%
group_by(Title) %>%
filter(n()>1) %>%
print() # This is actually two seperatre studies
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_gaming2 %>%
distinct(Title) %>%
count() %>%
print()
# Show duplicate:
results_gaming2 %>%
group_by(Title) %>%
filter(n()>1) %>%
print() # This is actually two seperatre studies
# Show duplicate:
results_gaming2 %>%
group_by(Title) %>%
filter(n()>1) %>%
print() # This is actually two seperatre studies
# Number of publications per year:
gaming_hist<- hist(results_gaming_sans_duplicates$Year,
xlim = c(1960,2023),
breaks = 40,
main = "Papers published per year on Gaming Disorder (PubMed)",
xlab = "Year")
# Number of separate journals:
results_gaming_sans_duplicates %>%
distinct(Journal_name_short) %>%
arrange(Journal_name_short) %>%
print(n = 20)
# Most popular journals:
results_gaming_sans_duplicates %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
# Now I need to add a label to all of these studies to signify that they Were returned from the gaming search so that we can distinguish them from other studies when we later joined the datasets together:
count(results_gaming_sans_duplicates)
Label <- rep("gaming", times = count(results_gaming_sans_duplicates))
results_gaming_final <- results_gaming_sans_duplicates %>% bind_cols(Label) %>%
rename(Label = 14) %>%
print()
# View(results_gaming_final)
# Now save the cleaned results
write.csv(results_gaming_final, "C:/Users/rheirene/Dropbox (Personal)/Sci-Software/Blog posts/PubMed Article Scraping/Web scrape scholar/Data extraction/gaming_data_cleaned.csv", row.names=FALSE)
# View(results_gaming_final)
# Now save the cleaned results
write.csv(results_gaming_final, "C:/Users/rheirene/Dropbox (Personal)/Sci-Software/Blog posts/PubMed Article Scraping/Web scrape scholar/Data extraction/gaming_data_cleaned.csv", row.names=FALSE)
# View(results_gaming_final)
# Now save the cleaned results
write.csv(results_gaming_final, "Data extraction/gaming_data_cleaned.csv", row.names=FALSE)
# Define the search terms/string:
search_term_work <- '("work addiction"[Title/Abstract]) OR ("addiction to work"[Title/Abstract]) OR ("workaholism"[Title/Abstract])'
# Use entrez_search to get the IDs of the articles:
search_results_work <- entrez_search(db="pubmed", term=search_term_work, retmax=20000)
id_list_work <- search_results_work$ids
# Split id list into chunks of 500:
chunks <- split(id_list_work, ceiling(seq_along(id_list_work)/500))
# Fetch details for each chunk of articles:
article_details_work <- map_dfr(chunks, function(ids) {
# Fetch details of the articles:
articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
# Split the articles into individual articles:
articles <- strsplit(articles, "\n\n")[[1]]
# Process each article:
map_dfr(articles, function(article) {
# Split the article into lines:
lines <- strsplit(article, "\n")[[1]]
# Get the details we're interested in:
details_work <- list(
PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
)
# Convert the list of details into a one-row data frame:
details_df_work <- bind_rows(details_work)
return(details_df_work)
})
})
# Check the results:
article_details_work %>%
as_tibble() %>%
print()
# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
'COIS- ' = '', 'JT  - ' = '')
results_work <- article_details_work %>%
mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
# Now separate the date year and month/day info:
separate(DP, c("Year", "Month"), sep = "\\ ") %>%
mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
rename(# Let's also provide more descriptive names for each of the columns
"Title" = "TI",
"DOI" = "LID",
"Abstract" = "AB",
"Full_Author_Name" = "FAU",
"Author_Address" = "AD",
"Language" = "LA",
"Publication_Type" = "PT",
"Journal_name_short" = "TA",
"Conflict_of_Interest_Statement" = "COIS",
"Journal_Title" = "JT") %>%
print()
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_work, "Data extraction/work_data.csv", row.names=FALSE)
# Get the total number of search results
results_count_gaming <- webpage_gaming_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
results_count_gaming
# Define the URL
url_work_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%22work+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addiction+to+work%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22workaholism%22%5BTitle%2FAbstract%5D%29&size=200"
# Read and parse the webpage
webpage_work_rvest <- read_html(url_work_rvest)
# Get the total number of search results
results_count_work <- webpage_work_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages
results_per_page_work <- 50
total_pages_work <- ceiling(results_count_work[1] / results_per_page_work)
# Print results_count_work and total_pages_work
print(results_count_work)
print(total_pages_work)
# Initialize an empty data frame
results_work_rvest <- data.frame()
# Loop through each page and scrape the data
for (page in 0:(total_pages_work - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_work_rvest, "&page=", page + 1)
# Read and parse the current webpage
current_page <- read_html(current_url)
# Extract title
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title info on page", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data
current_results_work <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results
results_work_rvest <- rbind(results_work_rvest, current_results_work)
# Introduce a delay to avoid overloading the server
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
# Print the extracted data
print(as_tibble(results_work_rvest))
# Now save the initial results before proceeding so we can't lose them!
write.csv(results_work_rvest, "Data extraction/work_titles.csv", row.names=FALSE)
# Load in first dataset in case environment clean on knit (above sections not excuted for render):
results_work<- read.csv("Data extraction/work_data.csv") %>%
as_tibble()
# Load in second dataset in case environment clean on knit (above sections not excuted for render):
results_work_rvest<- read.csv("Data extraction/work_titles.csv") %>%
as_tibble()
results_work
results_work_rvest
# Merge our two datasets:
results_work2 <- results_work %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_work_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_work2 %>%
distinct(Title) %>%
count() %>%
print() # None
# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_work2 %>%
distinct(PMID) %>%
count() %>%
print() # None
# Have a look at duplicates where the title, year, and authors are all the same:
results_work2 %>%
group_by(Title,
Journal_name_short,
Full_Author_Name) %>%
filter(n()>1)
print() # None
# Have a look at duplicates were just the title, journal and year are the same:
results_work2 %>%
group_by(Title,
Journal_name_short,
Year) %>%
filter(n()>1) %>%
print() # None
# Summary of the Year  variable:
summary(results_work2$Year)
# Number of publications per year:
work_hist<- hist(results_work2$Year,
xlim = c(1960,2023),
breaks = 40,
main = "Papers published per year on Work Addiction (PubMed)",
xlab = "Year")
# Number of separate journals:
results_work2 %>%
distinct(Journal_name_short) %>%
arrange(Journal_name_short) %>%
print(n = 20)
# Most popular journals:
results_work2 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
Label_work <- rep("work", times = count(results_work2))
results_work_final <- results_work2 %>% bind_cols(Label_work) %>%
rename(Label = 14) %>%
print()
# View(results_work_final)
# Now save the cleaned results
write.csv(results_work_final, "Data extraction/work_data_cleaned.csv", row.names=FALSE)
# Add search URL:
url_gambling_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%22gambling+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22disordered+gambling%22%5BTitle%2FAbstract%5D+OR+%22gambling+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addicted+gambler*%22%5BTitle%2FAbstract%5D%29+OR+%28%22pathological+gambl*%22%5BTitle%2FAbstract%5D%29+OR+%28%22compulsive+gambl*%22%5BTitle%2FAbstract%5D%29&size=200" # 3502 results on 10/07/2023
# Read and parse the webpage:
webpage_gambling_rvest <- read_html(url_gambling_rvest)
# Get the total number of search results:
results_count_gambling <- webpage_gambling_rvest %>%
html_node(".results-amount .value") %>%
html_text() %>%
str_replace(",", "") %>%
as.numeric()
# Calculate the number of pages:
results_per_page_gambling <- 100
total_pages_gambling <- ceiling(results_count_gambling[1] / results_per_page_gambling)
# Print results_count_gambling and total_pages_gambling:
print(results_count_gambling)
print(total_pages_gambling)
# Initialize an empty data frame:
results_gambling_rvest <- data.frame()
# Loop through each page and scrape the data:
for (page in 0:(total_pages_gambling - 1)) {
# Update the page parameter in the URL for the current page
current_url <- paste0(url_gambling_rvest, "&page=", page + 1)
# Read and parse the current webpage:
current_page <- read_html(current_url)
# Extract title:
Title <- tryCatch({
current_page %>%
html_nodes(".docsum-title") %>%
html_text(trim = TRUE)
}, error = function(e) {
print(paste("Error extracting title", page + 1))
rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
})
# Extract the PMID:
pmid <- current_page %>%
html_nodes(".docsum-pmid") %>%
html_text(trim = TRUE)
# Create a data frame with the extracted data:
current_results_gambling <- data.frame(Title = Title,
PMID = pmid)
# Combine the current results with the previous results:
results_gambling_rvest <- rbind(results_gambling_rvest, current_results_gambling)
# Introduce a delay to avoid overloading the server:
sleep_time <- runif(1, min = 5, max = 15)
Sys.sleep(sleep_time)
}
# Print the extracted data:
print(as_tibble(results_gambling_rvest))
# Now save the titles results before proceeding so we can't lose them!
write.csv(results_gambling_rvest, "Data extraction/gambling_titles.csv", row.names=FALSE)
