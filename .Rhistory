#     html_nodes(".docsum-pmid") %>%
#     html_text(trim = TRUE)
#
#   # Create a data frame with the extracted data
#   current_results_YouTube <- data.frame(Title = Title,
#                                        PMID = pmid)
#
#   # Combine the current results with the previous results
#   results_YouTube_rvest <- rbind(results_YouTube_rvest, current_results_YouTube)
#
#   # Introduce a delay to avoid overloading the server
#   sleep_time <- runif(1, min = 5, max = 15)
#   Sys.sleep(sleep_time)
# }
#
# # Print the extracted data
# print(as_tibble(results_YouTube_rvest))
#
#
# # Now save the titles before proceeding so we can't lose them!
# write.csv(results_YouTube_rvest, "Data extraction/YouTube_titles.csv", row.names=FALSE)
# Chunk 76
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_YouTube<- read.csv("Data extraction/YouTube_data.csv") %>%
as_tibble()
# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_YouTube_rvest<- read.csv("Data extraction/YouTube_titles.csv") %>%
as_tibble()
# Merge our two datasets
results_YouTube2 <- results_YouTube %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_YouTube_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Check results:
# View(results_YouTube2)
# Total number with basic duplication removal based on title:
results_YouTube2 %>%
distinct(Title) %>%
count() %>%
print() # n = 0 (August, 2023)
# Total number with basic duplication removal based on PMID:
results_YouTube2 %>%
distinct(PMID) %>%
count() %>%
print() # None
# Isolate relevant studies:
results_YouTube3 <-  results_YouTube2 %>%
filter(Title %in% c(
"YouTubeâ„¢ as an effective but potentially addictive distraction tool for paediatric phlebotomy.",
"Social media addiction: What is the role of content in YouTube?",
"Short Video Addiction on the Interaction of Creative Self-Efficacy and Career Interest to Innovative Design Profession Students."
))
# Summary of the year variable:
summary(results_YouTube3$Year)
# Number of publications per year:
YouTube_hist<- hist(results_YouTube3$Year,
xlim = c(1960,2023),
breaks = 5,
main = "Papers published per year on YouTube Addiction (PubMed)",
xlab = "Year")
# Most popular journals:
results_YouTube3 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
# View(results_YouTube2) # Visually inspect results
# Now I need to add a label to all of these studies to signify that they Were returned from the YouTube search so that we can distinguish them from other studies when we later joined the datasets together:
Label_YouTube <- rep("YouTube", times = count(results_YouTube3))
results_YouTube_final <- results_YouTube3 %>% bind_cols(Label_YouTube) %>%
rename(Label = 14)
# View(results_YouTube_final)
# Now save the cleaned results
write.csv(results_YouTube_final, "Data extraction/YouTube_data_cleaned.csv", row.names=FALSE)
# Chunk 77
# #  Clean environment:
# rm(list = ls())
#
# # Define the search term:
# search_term_death <- 'Addiction to death[Title/Abstract]'
#
# # Use entrez_search to get the IDs of the articles:
# search_results_death <- entrez_search(db="pubmed", term=search_term_death, retmax=20000)
# id_list_death <- search_results_death$ids
#
# # Split id list into chunks of 100 for chunk/batches:
# chunks <- split(id_list_death, ceiling(seq_along(id_list_death)/100))
#
# # Fetch details for each chunk of articles:
# article_details_death <- map_dfr(chunks, function(ids) {
#   # Fetch details of the articles:
#   articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
#
#   # Split the articles into individual articles:
#   articles <- strsplit(articles, "\n\n")[[1]]
#
#   # Process each article:
#   map_dfr(articles, function(article) {
#     # Split the article into lines:
#     lines <- strsplit(article, "\n")[[1]]
#
#     # Get the details we're interested in:
#     details_death <- list(
#       PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
#       DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
#       TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
#       LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
#       AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
#       FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
#       AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
#       LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
#       PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
#       TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
#       COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
#       JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
#     )
#
#     # Convert the list of details into a one-row data frame:
#     details_df_death <- bind_rows(details_death)
#
#     return(details_df_death)
#   })
# })
#
# # Check the results:
# article_details_death %>%
#   as_tibble() %>%
#   print()
#
# # Let's remove the identifiers at the beginning of each data point:
# rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
#              'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
#              'COIS- ' = '', 'JT  - ' = '')
#
# results_death <- article_details_death %>%
#   mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
#   # Now separate the date year and month/day info:
#   separate(DP, c("Year", "Month"), sep = "\\ ") %>%
#   mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
#   rename(# Let's also provide more descriptive names for each of the columns
#          "Title" = "TI",
#          "DOI" = "LID",
#          "Abstract" = "AB",
#          "Full_Author_Name" = "FAU",
#          "Author_Address" = "AD",
#          "Language" = "LA",
#          "Publication_Type" = "PT",
#          "Journal_name_short" = "TA",
#          "Conflict_of_Interest_Statement" = "COIS",
#          "Journal_Title" = "JT") %>%
#   print()
#
# # Now save the initial results before proceeding so we can't lose them!
# write.csv(results_death, "Data extraction/death_data.csv", row.names=FALSE)
# Chunk 78
# # Add search URL:
# url_death_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=Addiction+to+death%5BTitle%2FAbstract%5D&size=200" # 103 results returned on 20/08/2023
#
# # Read and parse the webpage
# webpage_death_rvest <- read_html(url_death_rvest)
#
# # Get the total number of search results
# results_count_death <- webpage_death_rvest %>%
#   html_node(".results-amount .value") %>%
#   html_text() %>%
#   str_replace(",", "") %>%
#   as.numeric()
#
# # Calculate the number of pages
# results_per_page_death <- 50
# total_pages_death <- ceiling(results_count_death[1] / results_per_page_death)
# # Print results_count_death and total_pages_death
# print(results_count_death)
# print(total_pages_death)
#
#
# # Initialize an empty data frame
# results_death_rvest <- data.frame()
#
# # Loop through each page and scrape the data
# for (page in 0:(total_pages_death - 1)) {
#   # Update the page parameter in the URL for the current page
#   current_url <- paste0(url_death_rvest, "&page=", page + 1)
#
#   # Read and parse the current webpage
#   current_page <- read_html(current_url)
#
#   # Extract title
#   Title <- tryCatch({
#     current_page %>%
#       html_nodes(".docsum-title") %>%
#       html_text(trim = TRUE)
#   }, error = function(e) {
#     print(paste("Error extracting title info on page", page + 1))
#     rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
#   })
#
#   # Extract the PMID
#   pmid <- current_page %>%
#     html_nodes(".docsum-pmid") %>%
#     html_text(trim = TRUE)
#
#   # Create a data frame with the extracted data
#   current_results_death <- data.frame(Title = Title,
#                                        PMID = pmid)
#
#   # Combine the current results with the previous results
#   results_death_rvest <- rbind(results_death_rvest, current_results_death)
#
#   # Introduce a delay to avoid overloading the server
#   sleep_time <- runif(1, min = 5, max = 15)
#   Sys.sleep(sleep_time)
# }
#
# # Print the extracted data
# print(as_tibble(results_death_rvest))
#
#
# # Now save the titles before proceeding so we can't lose them!
# write.csv(results_death_rvest, "Data extraction/death_titles.csv", row.names=FALSE)
# Chunk 79
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_death<- read.csv("Data extraction/death_data.csv") %>%
as_tibble()
# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_death_rvest<- read.csv("Data extraction/death_titles.csv") %>%
as_tibble()
# Merge our two datasets
results_death2 <- results_death %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_death_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Check results:
# View(results_death2)
# Total number with basic duplication removal based on title:
results_death2 %>%
distinct(Title) %>%
count() %>%
print() # n = 0 (August, 2023)
# Total number with basic duplication removal based on PMID:
results_death2 %>%
distinct(PMID) %>%
count() %>%
print() # None
# Isolate relevant study:
results_death3 <-  results_death2 %>%
filter(Title == "Addiction to death."
)
# Summary of the year variable:
summary(results_death3$Year)
# Number of publications per year:
death_hist<- hist(results_death3$Year,
xlim = c(1960,2023),
breaks = 100,
main = "Papers published per year on Death Addiction (PubMed)",
xlab = "Year")
# Most popular journals:
results_death3 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
# View(results_death2) # Visually inspect results
# Now I need to add a label to all of these studies to signify that they Were returned from the death search so that we can distinguish them from other studies when we later joined the datasets together:
Label_death <- rep("death", times = count(results_death3))
results_death_final <- results_death3 %>% bind_cols(Label_death) %>%
rename(Label = 14)
# View(results_death_final)
# Now save the cleaned results
write.csv(results_death_final, "Data extraction/death_data_cleaned.csv", row.names=FALSE)
# Chunk 80
# #  Clean environment:
# rm(list = ls())
#
# # Define the search term:
# search_term_near_death <- '"Addiction to near-death"'
#
# # Use entrez_search to get the IDs of the articles:
# search_results_near_death <- entrez_search(db="pubmed", term=search_term_near_death, retmax=20000)
# id_list_near_death <- search_results_near_death$ids
#
# # Split id list into chunks of 100 for chunk/batches:
# chunks <- split(id_list_near_death, ceiling(seq_along(id_list_near_death)/100))
#
# # Fetch details for each chunk of articles:
# article_details_near_death <- map_dfr(chunks, function(ids) {
#   # Fetch details of the articles:
#   articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
#
#   # Split the articles into individual articles:
#   articles <- strsplit(articles, "\n\n")[[1]]
#
#   # Process each article:
#   map_dfr(articles, function(article) {
#     # Split the article into lines:
#     lines <- strsplit(article, "\n")[[1]]
#
#     # Get the details we're interested in:
#     details_near_death <- list(
#       PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
#       DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
#       TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
#       LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
#       AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
#       FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
#       AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
#       LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
#       PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
#       TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
#       COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
#       JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
#     )
#
#     # Convert the list of details into a one-row data frame:
#     details_df_near_death <- bind_rows(details_near_death)
#
#     return(details_df_near_death)
#   })
# })
#
# # Check the results:
# article_details_near_death %>%
#   as_tibble() %>%
#   print()
#
# # Let's remove the identifiers at the beginning of each data point:
# rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
#              'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
#              'COIS- ' = '', 'JT  - ' = '')
#
# results_near_death <- article_details_near_death %>%
#   mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
#   # Now separate the date year and month/day info:
#   separate(DP, c("Year", "Month"), sep = "\\ ") %>%
#   mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
#   rename(# Let's also provide more descriptive names for each of the columns
#          "Title" = "TI",
#          "DOI" = "LID",
#          "Abstract" = "AB",
#          "Full_Author_Name" = "FAU",
#          "Author_Address" = "AD",
#          "Language" = "LA",
#          "Publication_Type" = "PT",
#          "Journal_name_short" = "TA",
#          "Conflict_of_Interest_Statement" = "COIS",
#          "Journal_Title" = "JT") %>%
#   print()
#
# # Now save the initial results before proceeding so we can't lose them!
# write.csv(results_near_death, "Data extraction/near_death_data.csv", row.names=FALSE)
# Chunk 81
# # Add search URL:
# url_near_death_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%22Addiction+to+near-death%22&size=200" # 103 results returned on 20/08/2023
#
# # Read and parse the webpage
# webpage_near_death_rvest <- read_html(url_near_death_rvest)
#
# # Get the total number of search results
# results_count_near_death <- webpage_near_death_rvest %>%
#   html_node(".results-amount .value") %>%
#   html_text() %>%
#   str_replace(",", "") %>%
#   as.numeric()
#
# # Calculate the number of pages
# results_per_page_near_death <- 50
# total_pages_near_death <- ceiling(results_count_near_death[1] / results_per_page_near_death)
# # Print results_count_near_death and total_pages_near_death
# print(results_count_near_death)
# print(total_pages_near_death)
#
#
# # Initialize an empty data frame
# results_near_death_rvest <- data.frame()
#
# # Loop through each page and scrape the data
# for (page in 0:(total_pages_near_death - 1)) {
#   # Update the page parameter in the URL for the current page
#   current_url <- paste0(url_near_death_rvest, "&page=", page + 1)
#
#   # Read and parse the current webpage
#   current_page <- read_html(current_url)
#
#   # Extract title
#   Title <- tryCatch({
#     current_page %>%
#       html_nodes(".docsum-title") %>%
#       html_text(trim = TRUE)
#   }, error = function(e) {
#     print(paste("Error extracting title info on page", page + 1))
#     rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
#   })
#
#   # Extract the PMID
#   pmid <- current_page %>%
#     html_nodes(".docsum-pmid") %>%
#     html_text(trim = TRUE)
#
#   # Create a data frame with the extracted data
#   current_results_near_death <- data.frame(Title = Title,
#                                        PMID = pmid)
#
#   # Combine the current results with the previous results
#   results_near_death_rvest <- rbind(results_near_death_rvest, current_results_near_death)
#
#   # Introduce a delay to avoid overloading the server
#   sleep_time <- runif(1, min = 5, max = 15)
#   Sys.sleep(sleep_time)
# }
#
# # Print the extracted data
# print(as_tibble(results_near_death_rvest))
#
#
# # Now save the titles before proceeding so we can't lose them!
# write.csv(results_near_death_rvest, "Data extraction/near_death_titles.csv", row.names=FALSE)
# Chunk 82
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_near_death<- read.csv("Data extraction/near_death_data.csv") %>%
as_tibble()
# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_near_death_rvest<- read.csv("Data extraction/near_death_titles.csv") %>%
as_tibble()
# Merge our two datasets
results_near_death2 <- results_near_death %>%
as_tibble() %>%
rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
full_join(results_near_death_rvest, by = "PMID") %>%
select(-Title_trunc) %>% # Bye truncated title
relocate(Title) %>% # Place full title first for ease
print()
# Check results:
# View(results_near_death2)
# Total number with basic duplication removal based on title:
results_near_death2 %>%
distinct(Title) %>%
count() %>%
print() # n = 0 (August, 2023)
# Total number with basic duplication removal based on PMID:
results_near_death2 %>%
distinct(PMID) %>%
count() %>%
print() # None
# Isolate relevant study:
results_near_death3 <-  results_near_death2 %>%
filter(Title == "Addiction to near-death."
)
# Summary of the year variable:
summary(results_near_death3$Year)
# Number of publications per year:
near_death_hist<- hist(results_near_death3$Year,
xlim = c(1960,2023),
breaks = 1,
main = "Papers published per year on Near Death Addiction (PubMed)",
xlab = "Year")
# Most popular journals:
results_near_death3 %>%
group_by(Journal_name_short) %>%
summarise(
n = n()
) %>%
arrange(desc(n)) %>%
print(n = 20)
# View(results_near_death2) # Visually inspect results
# Now I need to add a label to all of these studies to signify that they Were returned from the near_death search so that we can distinguish them from other studies when we later joined the datasets together:
Label_near_death <- rep("near_death", times = count(results_near_death3))
results_near_death_final <- results_near_death3 %>% bind_cols(Label_near_death) %>%
rename(Label = 14)
# View(results_near_death_final)
# Now save the cleaned results
write.csv(results_near_death_final, "Data extraction/near_death_data_cleaned.csv", row.names=FALSE)
# Chunk 83
combined_results_clean <- bind_rows(results_behav_addictions_final,
results_gambling_final,
results_gaming_final,
results_work_final,
results_study_final,
results_exercise_final,
results_social_media_final,
results_internet_final,
results_smart_phone_final,
results_cybersex_final,
results_sex_final,
results_pornography_final,
results_tanning_final,
results_hair_pulling_final,
results_love_final,
results_selfie_final,
results_extreme_sport_final,
results_running_final,
results_dance_final,
results_joyriding_final,
results_crime_final,
results_polysurgical_final,
results_fortune_telling_final,
results_YouTube_final,
results_death_final,
results_near_death_final
)
# INTERNET
# Now save the cleaned results
write.csv(combined_results_clean, "Data extraction/combined_results_clean.csv", row.names=FALSE)
