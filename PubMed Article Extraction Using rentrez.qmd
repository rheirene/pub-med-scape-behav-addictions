---
title: "PubMed Article Extraction Using rentrez"
author: "Rob Heirene"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: 
  html:
    toc: true
editor: visual
---

## Preamble

The criteria for inclusion are:

-   The article must be registered/stored on the PubMed database (a list of journals indexed on PubMed can be found [here](https://ftp.ncbi.nih.gov/pubmed/J_Medline.txt)).

<!-- -->

-   The article must focus on a behaviour which has been studied within the context of addiction (e.g., gambling, exercise, video gaming); the behaviour can be the primary focus or a sub-component of a larger paper (e.g., a study on alcohol use which also measures symptoms of gambling disorder)

-   The article must use language in the title and/or abstract that indicate that the behaviour is discussed in the context of or as an addiction. This could include, but is not restricted to:

    -   the direct use of the terms "addiction" or "dependence"

    -   the use of accepted (DSM/ICD) or previosuly accepted (e.g., "pathological gambling") diagnostic terms (e.g., "gambling disorder")

    -   the use of a term or phrase which is not a clinically/medically accepted diagnostic term (i.e., it is not included in any nosological system), but is used in the research area as such (e.g., "compulsory exercise", "workaholism")

    -   the use of measures of "addiction" to the behaviour (e.g., "Exercise Addiction Inventory") or measures of the symptoms of addiction (e.g., craving, withdrawal) that are then discussed in the context of a disorder or addiction

-   There are no restrictions on date of publication, study designs, article types (e.g., commentaries, empirical studies etc.)

-   There are no restrictions on language publication, provided the title and abstract are presented in English as well as the primary language.

The above criteria were used to design of the search terms/strings employed to extract data from the PubMed database (the search string for each behavioural addiction is included before the code used to extract the associated studies). A manual scan of all return studies for relevance according to the above criteria has been made.

https://www.youtube.com/watch?v=yzTuBuRdAyA

::: {.callout-note appearance="minimal"}
## Code blocks

Please note that any code chunks presented in this document that, when executed, scrape or pull data from PubMed, are specifically set so that they do not execute when this document renders. Thus, these code chunks, as seen here, simply represent documentation rather than working code. This is because extracting this data (particularly for behavioural addictions where there are thousands of studies \[e.g., gambling\]) can take a substantial amount of time and therefore it would take 30-40 minutes every time this document renders if these code chunks executed.

I only note this for anyone wanting to reproduce this analysis who tries to run the entire script by rendering the document. It's also important as the date at the top of this document may not be the latest date of data extraction (it is only the last date this was rendered).

The latest dates of data extraction for each behavioural addiction are below:

-   Gambling: 10/07/2023
-   Gaming: 10/07/2023
-   Work: 10/07/2023
-   Exercise: 10/07/2023
:::

https://cran.r-project.org/web/packages/rentrez/vignettes/rentrez_tutorial.html

## Load packages

```{r results=FALSE, warning=FALSE, message=FALSE}
#| code-fold: true
#| code-summary: "Set-up code"

# Install and load the groundhog package to ensure consistency of the package versions used here:
# install.packages("groundhog") # Install

library(groundhog) # Load

# List desired packages:
packages <- c('rentrez',
              'dplyr',
              'tidyr',
              'purrr',
              'rvest',
              'stringr')
              

# Load desired package with versions specific to project start date:
groundhog.library(packages, "2023-06-07")
```

## Behavioural addictions

#### Extract studies and perform a basic clean of dataset

**Notes pre-search:** MeSH terms were pretty useless here (unlike the more specific behavioural addiction searches used below) so I relied on my knowledge of the field for terms to use.

**Actual search used:** (((("behavioural addiction"\[Title/Abstract\]) OR ("behavioral addiction"\[Title/Abstract\])) OR ("non-drug addiction"\[Title/Abstract\])) OR ("non-substance addiction"\[Title/Abstract\])) OR ("non-chemical addiction"\[Title/Abstract\])

**Manual search results:** [link](https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%22behavioural+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22behavioral+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-drug+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-substance+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-chemical+addiction%22%5BTitle%2FAbstract%5D%29&size=200). PubMed format display can be found [here](https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%22behavioural+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22behavioral+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-drug+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-substance+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-chemical+addiction%22%5BTitle%2FAbstract%5D%29&format=pubmed&size=200)

**Notes post-search:** I manually scanned the results returned on the PubMed site for relevance and all appeared to meet the inclusion criteria. There will certainly be one of overlap between the studies returned from this search and the search is specific to individual behavioural addictions.

```{{r}}
#  Clean environment:
# rm(list = ls())

# Define the search terms/string:
search_term_behav_addictions <- '(((("behavioural addiction"[Title/Abstract]) OR ("behavioral addiction"[Title/Abstract])) OR ("non-drug addiction"[Title/Abstract])) OR ("non-substance addiction"[Title/Abstract])) OR ("non-chemical addiction"[Title/Abstract])'

# Use entrez_search to get the IDs of the articles:
search_results_behav_addictions <- entrez_search(db="pubmed", term=search_term_behav_addictions, retmax=20000)
id_list_behav_addictions <- search_results_behav_addictions$ids

# Split id list into chunks of 500:
chunks <- split(id_list_behav_addictions, ceiling(seq_along(id_list_behav_addictions)/100))

# Fetch details for each chunk of articles:
article_details_behav_addictions <- map_dfr(chunks, function(ids) {
  articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
  
  # Split the articles into individual articles:
  articles <- strsplit(articles, "\n\n")[[1]]
  
  # Process each article:
  map_dfr(articles, function(article) {
    # Split the article into lines:
    lines <- strsplit(article, "\n")[[1]]
    
    # Get the details we're interested in:
    details_behav_addictions <- list(
      PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
      DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
      TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
      LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
      AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
      FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
      AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
      LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
      PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
      TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
      COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
      JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
    )
    
    # Convert the list of details into a one-row data frame:
    details_df_behav_addictions <- bind_rows(details_behav_addictions)
    
    return(details_df_behav_addictions)
  })
})

# Check the results:
article_details_behav_addictions %>%
  as_tibble() %>%
  print()

# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
             'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
             'COIS- ' = '', 'JT  - ' = '')

results_behav_addictions <- article_details_behav_addictions %>%
  mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
  # Now separate the date year and month/day info:
  separate(DP, c("Year", "Month"), sep = "\\ ") %>%
  mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
  rename( # Let's also provide more descriptive names for each of the columns
         "Title" = "TI", 
         "DOI" = "LID", 
         "Abstract" = "AB", 
         "Full_Author_Name" = "FAU", 
         "Author_Address" = "AD", 
         "Language" = "LA", 
         "Publication_Type" = "PT", 
         "Journal_name_short" = "TA",
         "Conflict_of_Interest_Statement" = "COIS",
         "Journal_Title" = "JT") %>%
  print()

# Now save the initial results before proceeding so we can't lose them!
write.csv(results_behav_addictions, "Data extraction/behav_addictions_data.csv", row.names=FALSE)

```

#### Extract full title using `rvest`

Okay, so I'm struggling to get the `rentrez` package to extract the full, non-truncated titles of articles. I want to make sure we have these so we can accurately identify duplicates, so we're going to use the `rvest` package to scrape the titles returned from a manual search in PubMed (it uses the URL from this search), then join the two datasets using the PMID, retaining the title from this new dataset.

::: {.callout-note appearance="minimal"}
## Using the `revest` package

It's important to note that, unlike using the `rentrez` package X uses data via PubMed's API, using the `rvest` package involves scraping the PubMed website for information, and there are many important ethical and legal considerations surrounding web scraping (some of which vary from country to country) that you may need to consider before proceeding. See, for example, this useful [paper](https://www.researchgate.net/profile/Vlad-Krotov/publication/324907302_Legality_and_Ethics_of_Web_Scraping/links/5aea622345851588dd8287dc/Legality-and-Ethics-of-Web-Scraping.pdf).
:::

```{{r}}
# Add search URL:
url_behav_addictions_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%22behavioural+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22behavioral+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-drug+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-substance+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22non-chemical+addiction%22%5BTitle%2FAbstract%5D%29&size=200" # 856 results on 11/07/2023. 

# Read and parse the webpage:
webpage_behav_addictions_rvest <- read_html(url_behav_addictions_rvest)

# Get the total number of search results:
results_count_behav_addictions <- webpage_behav_addictions_rvest %>%
  html_node(".results-amount .value") %>%
  html_text() %>%
  str_replace(",", "") %>%
  as.numeric()

# Calculate the number of pages:
results_per_page_behav_addictions <- 100
total_pages_behav_addictions <- ceiling(results_count_behav_addictions[1] / results_per_page_behav_addictions)
# Print results_count_behav_addictions and total_pages_behav_addictions:
print(results_count_behav_addictions)
print(total_pages_behav_addictions)


# Initialize an empty data frame:
results_behav_addictions_rvest <- data.frame()

# Loop through each page and scrape the data:
for (page in 0:(total_pages_behav_addictions - 1)) {
  # Update the page parameter in the URL for the current page
  current_url <- paste0(url_behav_addictions_rvest, "&page=", page + 1)
  
  # Read and parse the current webpage:
  current_page <- read_html(current_url)
  
  # Extract title:
  Title <- tryCatch({
    current_page %>%
      html_nodes(".docsum-title") %>%
      html_text(trim = TRUE)
  }, error = function(e) {
    print(paste("Error extracting title", page + 1))
    rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
  })
  
  # Extract the PMID:
  pmid <- current_page %>%
    html_nodes(".docsum-pmid") %>%
    html_text(trim = TRUE)
  
  # Create a data frame with the extracted data:
  current_results_behav_addictions <- data.frame(Title = Title,
                                       PMID = pmid)
  
  # Combine the current results with the previous results:
  results_behav_addictions_rvest <- rbind(results_behav_addictions_rvest, current_results_behav_addictions)
  
  # Introduce a delay to avoid overloading the server:
  sleep_time <- runif(1, min = 5, max = 15)
  Sys.sleep(sleep_time)
}

# Print the extracted data:
print(as_tibble(results_behav_addictions_rvest))

# Now save the titles results before proceeding so we can't lose them!
write.csv(results_behav_addictions_rvest, "Data extraction/behav_addictions_titles.csv", row.names=FALSE)
```

#### Refine dataset

Now we need to identify and remove any duplicates, irrelevant studies, or entries with any other issues (e.g., those that turn out to be books)

```{r}
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_behav_addictions<- read.csv("Data extraction/behav_addictions_data.csv") %>% 
  as_tibble()

# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_behav_addictions_rvest<- read.csv("Data extraction/behav_addictions_titles.csv") %>% 
  as_tibble()

# Merge the two datasets:
results_behav_addictions2 <- results_behav_addictions %>%
  rename("Title_trunc" = "Title") %>% # Designate the truncated title from the primary dataset
  merge(results_behav_addictions_rvest, by = "PMID") %>%
  relocate(Title) %>% # Place full title first for ease
  as_tibble() %>%
  print()

# Check results:
# View(results_behav_addictions2)

# Check dupliactes based on PMIDs:
results_behav_addictions2 %>%
  group_by(PMID) %>%
  filter(n()>1) %>% 
  print(n = 100)

# There is no evidence of clear PMID duplicates

# Show duplicate titles:
results_behav_addictions2 %>% 
  group_by(Title) %>% 
  filter(n()>1) 
# **This is actually two seperate commentaries on the same article**

#  Okay, now we are confident that we have a clean dataset, let's explore!

# Summary of the Year  variable:
summary(results_behav_addictions2$Year)

# Number of publications per year:
hist(results_behav_addictions2$Year,
                     xlim = c(1960,2023),
                     breaks = 30,
                     main = "Papers published per year on 'behavioural addictions' (PubMed)",
                     xlab = "Year") # Even though the earliest year an article was published is 1994, I've set the range to started 1960 as this is where the starting point is across all other behavioural addiction searches.


# Number of separate journals:
results_behav_addictions2 %>% 
  distinct(Journal_name_short) %>% 
  arrange(Journal_name_short) %>% 
  print(n = 50)

# Most popular journals:
results_behav_addictions2 %>% 
  group_by(Journal_name_short) %>% 
  summarise(
    n = n()
  ) %>% 
  arrange(desc(n)) %>%
  print(n = 50)

# View(results_behav_addictions_sans_duplicates) # Visually inspect results

# Now I need to add a label to all of these studies to signify that they Were returned from the behav_addictions search so that we can distinguish them from other studies when we later joined the datasets together:
Label <- rep("behavioural_addictions", times = count(results_behav_addictions2))

results_behav_addictions_final <- results_behav_addictions2 %>% bind_cols(Label) %>%
  rename(Label = 14) %>%
  print()

# View(results_behav_addictions_final)
# Now save the cleaned results:
write.csv(results_behav_addictions_final, "Data extraction/behav_addictions_data_cleaned.csv", row.names=FALSE)
```

## Gambling addiction

#### Extract studies and perform a basic clean of dataset

**Notes pre-search:** Mesh terms can be found [here](https://www.ncbi.nlm.nih.gov/mesh/?term=gaming+disorder). I tried adding "problem gambling" and it adds around 1,200 more studies, but the overall distribution over time remains very similar.

-   One unavoidable issue with the gambling search is 2 of the 3 gambling-specific journals (International Gambling Studies & Journal of Gambling Issues) are not indexed by PubMed. That said, most behavioural addiction and addiction-focused journals are, fortunately.

::: {.callout-note appearance="minimal" icon="false"}
## Add gambling journals?

A simple reminder to look into this.
:::

**Actual search used:** ("gambling disorder"\[Title/Abstract\]) OR ("disordered gambling"\[Title/Abstract\] OR "gambling addiction"\[Title/Abstract\]) OR ("addicted gambler\*"\[Title/Abstract\]) OR ("pathological gambl\*"\[Title/Abstract\]) OR ("compulsive gambl\*"\[Title/Abstract\]

**Manual search results:** [link](https://pubmed.ncbi.nlm.nih.gov/?term=%28%22gambling+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22disordered+gambling%22%5BTitle%2FAbstract%5D+OR+%22gambling+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addicted+gambler*%22%5BTitle%2FAbstract%5D%29+OR+%28%22pathological+gambl*%22%5BTitle%2FAbstract%5D%29+OR+%28%22compulsive+gambl*%22%5BTitle%2FAbstract%5D%29&size=200)

**Notes post-search:** I manually inspected the results directly in the PubMed site. Many duplicates that require removal (performed below), but all studies appeared relevant (July 2023).

```{{r}}
#  Clean environment:
rm(list = ls())

# Define the search terms/string:
search_term_gambling <- '("gambling disorder"[Title/Abstract]) OR ("disordered gambling"[Title/Abstract] OR "gambling addiction"[Title/Abstract]) OR ("addicted gambler*"[Title/Abstract]) OR ("pathological gambl*"[Title/Abstract]) OR ("compulsive gambl*"[Title/Abstract])'

# Use entrez_search to get the IDs of the articles:
search_results_gambling <- entrez_search(db="pubmed", term=search_term_gambling, retmax=20000)
id_list_gambling <- search_results_gambling$ids

# Split id list into chunks of 500:
chunks <- split(id_list_gambling, ceiling(seq_along(id_list_gambling)/100))

# Fetch details for each chunk of articles:
article_details_gambling <- map_dfr(chunks, function(ids) {
  articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
  
  # Split the articles into individual articles:
  articles <- strsplit(articles, "\n\n")[[1]]
  
  # Process each article:
  map_dfr(articles, function(article) {
    # Split the article into lines:
    lines <- strsplit(article, "\n")[[1]]
    
    # Get the details we're interested in:
    details_gambling <- list(
      PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
      DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
      TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
      LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
      AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
      FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
      AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
      LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
      PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
      TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
      COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
      JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
    )
    
    # Convert the list of details into a one-row data frame:
    details_df_gambling <- bind_rows(details_gambling)
    
    return(details_df_gambling)
  })
})

# Check the results:
article_details_gambling %>%
  as_tibble() %>%
  print()

# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
             'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
             'COIS- ' = '', 'JT  - ' = '')

results_gambling <- article_details_gambling %>%
  mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
  # Now separate the date year and month/day info:
  separate(DP, c("Year", "Month"), sep = "\\ ") %>%
  mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
  rename( # Let's also provide more descriptive names for each of the columns
         "Title" = "TI", 
         "DOI" = "LID", 
         "Abstract" = "AB", 
         "Full_Author_Name" = "FAU", 
         "Author_Address" = "AD", 
         "Language" = "LA", 
         "Publication_Type" = "PT", 
         "Journal_name_short" = "TA",
         "Conflict_of_Interest_Statement" = "COIS",
         "Journal_Title" = "JT") %>%
  print()

# Now save the initial results before proceeding so we can't lose them!
write.csv(results_gambling, "Data extraction/gambling_data.csv", row.names=FALSE)

```

#### Extract full title using rvest

As above, we're going to use the `rvest` package to scrape the titles of gambling papers, then join the two datasets using the PMID, retaining the title from this new dataset.

```{{r}}
# Add search URL:
url_gambling_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%22gambling+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22disordered+gambling%22%5BTitle%2FAbstract%5D+OR+%22gambling+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addicted+gambler*%22%5BTitle%2FAbstract%5D%29+OR+%28%22pathological+gambl*%22%5BTitle%2FAbstract%5D%29+OR+%28%22compulsive+gambl*%22%5BTitle%2FAbstract%5D%29&size=200" # 3502 results on 10/07/2023

# Read and parse the webpage:
webpage_gambling_rvest <- read_html(url_gambling_rvest)

# Get the total number of search results:
results_count_gambling <- webpage_gambling_rvest %>%
  html_node(".results-amount .value") %>%
  html_text() %>%
  str_replace(",", "") %>%
  as.numeric()

# Calculate the number of pages:
results_per_page_gambling <- 100
total_pages_gambling <- ceiling(results_count_gambling[1] / results_per_page_gambling)
# Print results_count_gambling and total_pages_gambling:
print(results_count_gambling)
print(total_pages_gambling)


# Initialize an empty data frame:
results_gambling_rvest <- data.frame()

# Loop through each page and scrape the data:
for (page in 0:(total_pages_gambling - 1)) {
  # Update the page parameter in the URL for the current page
  current_url <- paste0(url_gambling_rvest, "&page=", page + 1)
  
  # Read and parse the current webpage:
  current_page <- read_html(current_url)
  
  # Extract title:
  Title <- tryCatch({
    current_page %>%
      html_nodes(".docsum-title") %>%
      html_text(trim = TRUE)
  }, error = function(e) {
    print(paste("Error extracting title", page + 1))
    rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
  })
  
  # Extract the PMID:
  pmid <- current_page %>%
    html_nodes(".docsum-pmid") %>%
    html_text(trim = TRUE)
  
  # Create a data frame with the extracted data:
  current_results_gambling <- data.frame(Title = Title,
                                       PMID = pmid)
  
  # Combine the current results with the previous results:
  results_gambling_rvest <- rbind(results_gambling_rvest, current_results_gambling)
  
  # Introduce a delay to avoid overloading the server:
  sleep_time <- runif(1, min = 5, max = 15)
  Sys.sleep(sleep_time)
}

# Print the extracted data:
print(as_tibble(results_gambling_rvest))

# Now save the titles results before proceeding so we can't lose them!
write.csv(results_gambling_rvest, "Data extraction/gambling_titles.csv", row.names=FALSE)
```

#### Refine dataset

Now we need to identify and remove any duplicates, irrelevant studies, or entries with any other issues (e.g., those that turn out to be books)

```{r}
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_gambling<- read.csv("Data extraction/gambling_data.csv") %>% 
  as_tibble()

# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_gambling_rvest<- read.csv("Data extraction/gambling_titles.csv") %>% 
  as_tibble()

# Merge the two datasets:
results_gambling2 <- results_gambling %>%
  rename("Title_trunc" = "Title") %>% # Designate the truncated title from the primary dataset
  merge(results_gambling_rvest, by = "PMID") %>%
  relocate(Title) %>% # Place full title first for ease
  as_tibble() %>%
  print()

# Check results:
# View(results_gambling2)

# Check dupliactes based on PMIDs:
Simple_duplicate_removal_n <- results_gambling2 %>%
  distinct(PMID) %>%
  count() %>% 
  print()

results_gambling2 %>%
  group_by(PMID) %>%
  filter(n()>1) %>% 
  print(n = 100)

# Remove these asap as they're clear duplicates:
results_gambling_sans_duplicates<- results_gambling2 %>%
  distinct(PMID, .keep_all = TRUE) %>% 
  print()

# Total number with basic duplication removal based on title (there are some distinct papers with the same title like "gambling disorder", so this is too simplified, but it'll do for a quick check):
Simple_duplicate_removal_n <- results_gambling2 %>%
  distinct(Title) %>%
  count() %>% 
  print()

# How many duplicates (based on title alone) does this remove?
count(results_gambling_sans_duplicates) - Simple_duplicate_removal_n 

# Have a look at duplicates where the title, year, and authors are all the same:
results_gambling_sans_duplicates %>% 
  group_by(Title, Journal_name_short, Full_Author_Name) %>% 
  filter(n()>1) %>% 
  print()

# Have a look at duplicates were just the title, journal and year are the same:
results_gambling_sans_duplicates %>% 
  group_by(Title, Journal_name_short, Year) %>% 
  filter(n()>1) %>% 
  # View()
  print()

# There are a few more of these, suggesting there are some papers with the same title, journal and date but different authors. 
# The only one of concern is this one (PMID: 31346181) which is  agraphical abstract for another paper by the looks. 

# Remove duplicates based on above observation:
results_gambling_sans_duplicates2<- results_gambling_sans_duplicates %>%
filter(PMID != "31346181") %>% 
  print()

# Check this has removed the duplicate:
count(results_gambling_sans_duplicates) - count(results_gambling_sans_duplicates2)

#  Okay, now we have a clean dataset, let's explore!

# Summary of the Year  variable:
summary(results_gambling_sans_duplicates2$Year)

# Number of publications per year:
Gambling_hist<- hist(results_gambling_sans_duplicates2$Year,
                     xlim = c(1960,2023),
                     breaks = 60,
                     main = "Papers published per year on Gambling Disorder (PubMed)",
                     xlab = "Year")


# Number of separate journals:
results_gambling_sans_duplicates2 %>% 
  distinct(Journal_name_short) %>% 
  arrange(Journal_name_short) %>% 
  print(n = 50)

# Most popular journals:
results_gambling_sans_duplicates2 %>% 
  group_by(Journal_name_short) %>% 
  summarise(
    n = n()
  ) %>% 
  arrange(desc(n)) %>%
  print(n = 50)

# View(results_gambling_sans_duplicates) # Visually inspect results

# Now I need to add a label to all of these studies to signify that they Were returned from the gambling search so that we can distinguish them from other studies when we later joined the datasets together:
count(results_gambling_sans_duplicates2)

Label <- rep("gambling", times = count(results_gambling_sans_duplicates2))

results_gambling_final <- results_gambling_sans_duplicates2 %>% bind_cols(Label) %>%
  rename(Label = 14) %>%
  print()

# View(results_gambling_final)
# Now save the cleaned results:
write.csv(results_gambling_final, "Data extraction/gambling_data_cleaned.csv", row.names=FALSE)
```

## Gaming addiction

#### Extract studies and perform a basic clean of dataset

**Notes pre-search:** Mesh terms can be found [here](https://www.ncbi.nlm.nih.gov/mesh/?term=gaming+disorder). Didn't use all of these terms as some relate to other behavioural addictions

**Actual search used:** ((((("gaming disorder"\[Title/Abstract\]) OR ("internet gaming disorder"\[Title/Abstract\])) OR ("gaming addiction"\[Title/Abstract\])) OR ("video game addiction"\[Title/Abstract\])) OR ("video game disorder"\[Title/Abstract\])) OR ("gaming dependence")

**Manual search results:** [link](https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%28%22gaming+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22internet+gaming+disorder%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22gaming+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22video+game+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22video+game+disorder%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22gaming+dependence%22%29&size=200)

**Notes post-search:** "video game dependence" returned no results and so was removed. I manually inspected the results directly in the PubMed site and all appeared relevant (July 2023).

```{{r}}
#  Clean environment:
# rm(list = ls())

# Define the search terms/string:
search_term_gaming <- '((((("gaming disorder"[Title/Abstract]) OR ("internet gaming disorder"[Title/Abstract])) OR ("gaming addiction"[Title/Abstract])) OR ("video game addiction"[Title/Abstract])) OR ("video game disorder"[Title/Abstract])) OR ("gaming dependence")'

# Use entrez_search to get the IDs of the articles:
search_results_gaming <- entrez_search(db="pubmed", term=search_term_gaming, retmax=20000)
id_list_gaming <- search_results_gaming$ids

# Split id list into chunks of 500:
chunks <- split(id_list_gaming, ceiling(seq_along(id_list_gaming)/100))

# Fetch details for each chunk of articles:
article_details_gaming <- map_dfr(chunks, function(ids) {
  # Fetch details of the articles:
  articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
  
  # Split the articles into individual articles:
  articles <- strsplit(articles, "\n\n")[[1]]
  
  # Process each article:
  map_dfr(articles, function(article) {
    # Split the article into lines:
    lines <- strsplit(article, "\n")[[1]]
    
    # Get the details we're interested in:
    details_gaming <- list(
      PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
      DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
      TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
      LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
      AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
      FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
      AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
      LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
      PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
      TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
      COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
      JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
    )
    
    # Convert the list of details into a one-row data frame:
    details_df_gaming <- bind_rows(details_gaming)
    
    return(details_df_gaming)
  })
})

# Check the results:
article_details_gaming %>%
  as_tibble() %>%
  print()

# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
             'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
             'COIS- ' = '', 'JT  - ' = '')

results_gaming <- article_details_gaming %>%
  mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
  # Now separate the date year and month/day info:
  separate(DP, c("Year", "Month"), sep = "\\ ") %>%
  mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
  rename(# Let's also provide more descriptive names for each of the columns
         "Title" = "TI", 
         "DOI" = "LID", 
         "Abstract" = "AB", 
         "Full_Author_Name" = "FAU", 
         "Author_Address" = "AD", 
         "Language" = "LA", 
         "Publication_Type" = "PT", 
         "Journal_name_short" = "TA",
         "Conflict_of_Interest_Statement" = "COIS",
         "Journal_Title" = "JT") %>%
  print()

# Now save the initial results before proceeding so we can't lose them!
write.csv(results_gaming, "Data extraction/gaming_data.csv", row.names=FALSE)

```

#### Extract full title using rvest

As with the gambling study extraction, we're going to use the `rvest` package to scrape the titles returned from the manual search in PubMed, then join the two datasets using the PMID, retaining the title from this new dataset.

```{{r}}
# Add search URL:
url_gaming_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%28%22gaming+disorder%22%5BTitle%2FAbstract%5D%29+OR+%28%22internet+gaming+disorder%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22gaming+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22video+game+addiction%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22video+game+disorder%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22gaming+dependence%22%29&size=200"

# Read and parse the webpage
webpage_gaming_rvest <- read_html(url_gaming_rvest)

# Get the total number of search results
results_count_gaming <- webpage_gaming_rvest %>%
  html_node(".results-amount .value") %>%
  html_text() %>%
  str_replace(",", "") %>%
  as.numeric()

# Calculate the number of pages
results_per_page_gaming <- 50
total_pages_gaming <- ceiling(results_count_gaming[1] / results_per_page_gaming)
# Print results_count_gaming and total_pages_gaming
print(results_count_gaming)
print(total_pages_gaming)


# Initialize an empty data frame
results_gaming_rvest <- data.frame()

# Loop through each page and scrape the data
for (page in 0:(total_pages_gaming - 1)) {
  # Update the page parameter in the URL for the current page
  current_url <- paste0(url_gaming_rvest, "&page=", page + 1)
  
  # Read and parse the current webpage
  current_page <- read_html(current_url)
  
  # Extract title
  Title <- tryCatch({
    current_page %>%
      html_nodes(".docsum-title") %>%
      html_text(trim = TRUE)
  }, error = function(e) {
    print(paste("Error extracting title info on page", page + 1))
    rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
  })
  
  # Extract the PMID:
  pmid <- current_page %>%
    html_nodes(".docsum-pmid") %>%
    html_text(trim = TRUE)
  
  # Create a data frame with the extracted data:
  current_results_gaming <- data.frame(Title = Title,
                                       PMID = pmid)
  
  # Combine the current results with the previous results:
  results_gaming_rvest <- rbind(results_gaming_rvest, current_results_gaming)
  
  # Introduce a delay to avoid overloading the server:
  sleep_time <- runif(1, min = 5, max = 15)
  Sys.sleep(sleep_time)
}

# Print the extracted data:
print(as_tibble(results_gaming_rvest))

# Now save the titles before proceeding so we can't lose them!
write.csv(results_gaming_rvest, "Data extraction/gaming_titles.csv", row.names=FALSE)
```

#### Refine dataset

Now we need to identify and remove any duplicates, irrelevant studies, or entries with any other issues (e.g., those that turn out to be books)

```{r}

# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_gaming<- read.csv("Data extraction/gaming_data.csv") %>% 
  as_tibble()

# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_gaming_rvest<- read.csv("Data extraction/gaming_titles.csv") %>% 
  as_tibble()

# Merge our two datasets:
results_gaming2 <- results_gaming %>%
  as_tibble() %>%
  rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
  full_join(results_gaming_rvest, by = "PMID") %>%
  select(-Title_trunc) %>% # Bye truncated title
  relocate(Title) %>% # Place full title first for ease
  print()

# Check results:
# View(results_gaming2)

# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_gaming2 %>%
  distinct(Title) %>%
  count() %>% 
  print()

# Show duplicate:
results_gaming2 %>% 
  group_by(Title) %>% 
  filter(n()>1) # **This is actually two seperate studies**

 
# I know from manual searching that there is a genuine duplicate that is a corrigendum to one study. The PMID  for the corrigendum is 35543161 (Remove later)

# Have a look at duplicates where the title, year, and authors are all the same:
results_gaming2 %>% 
  group_by(Title, 
           Journal_name_short, 
           Full_Author_Name) %>% 
  filter(n()>1)


# Have a look at duplicates were just the title, journal and year are the same:
results_gaming2 %>% 
  group_by(Title, 
           Journal_name_short, 
           Year) %>% 
  filter(n()>1) 


# Remove the duplicate study:
results_gaming_sans_duplicates<- results_gaming2 %>%
filter(PMID != "35543161") %>% 
  print()

# Check this has just remove one study
count(results_gaming2) - count(results_gaming_sans_duplicates) # Yep

#  Okay, now we have a clean dataset, let's explore!

# Summary of the Year  variable:
summary(results_gaming_sans_duplicates$Year)

# Number of publications per year:
gaming_hist<- hist(results_gaming_sans_duplicates$Year,
                     xlim = c(1960,2023),
                     breaks = 40,
                     main = "Papers published per year on Gaming Disorder (PubMed)",
                     xlab = "Year")

# Number of separate journals:
results_gaming_sans_duplicates %>% 
  distinct(Journal_name_short) %>% 
  arrange(Journal_name_short) %>% 
  print(n = 20)

# Most popular journals:
results_gaming_sans_duplicates %>% 
  group_by(Journal_name_short) %>% 
  summarise(
    n = n()
  ) %>% 
  arrange(desc(n)) %>%
  print(n = 20)

# View(results_gaming_sans_duplicates) # Visually inspect results

# Now I need to add a label to all of these studies to signify that they Were returned from the gaming search so that we can distinguish them from other studies when we later joined the datasets together:
count(results_gaming_sans_duplicates)

Label <- rep("gaming", times = count(results_gaming_sans_duplicates))

results_gaming_final <- results_gaming_sans_duplicates %>% bind_cols(Label) %>%
  rename(Label = 14) %>%
  print()

# View(results_gaming_final)
# Now save the cleaned results:
write.csv(results_gaming_final, "Data extraction/gaming_data_cleaned.csv", row.names=FALSE)
```

## Work addiction

#### Extract studies and perform a basic clean of dataset

**Notes pre-search:** Some key terms I have found in the literature are "work addiction" and \[https://pubmed.ncbi.nlm.nih.gov/30920291/\] "workaholism" \[https://pubmed.ncbi.nlm.nih.gov/28425778/\]

**Actual search used:** (("work addiction"\[Title/Abstract\]) OR ("addiction to work"\[Title/Abstract\])) OR ("workaholism"\[Title/Abstract\])

**Manual search results:** [link](https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%22work+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addiction+to+work%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22workaholism%22%5BTitle%2FAbstract%5D%29&size=200)

**Notes post-search:** Again, I manually inspected all of the results directly in the PubMed site. All appeared relevant other than one study which is mentioned and removed in the code under the title "Refine dataset" within this section.

```{{r}}
#  Clean environment:
rm(list = ls())

# Define the search term:
search_term_work <- '("work addiction"[Title/Abstract]) OR ("addiction to work"[Title/Abstract]) OR ("workaholism"[Title/Abstract])'

# Use entrez_search to get the IDs of the articles:
search_results_work <- entrez_search(db="pubmed", term=search_term_work, retmax=20000)
id_list_work <- search_results_work$ids

# Split id list into chunks of 500:
chunks <- split(id_list_work, ceiling(seq_along(id_list_work)/500))

# Fetch details for each chunk of articles:
article_details_work <- map_dfr(chunks, function(ids) {
  # Fetch details of the articles:
  articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
  
  # Split the articles into individual articles:
  articles <- strsplit(articles, "\n\n")[[1]]
  
  # Process each article:
  map_dfr(articles, function(article) {
    # Split the article into lines:
    lines <- strsplit(article, "\n")[[1]]
    
    # Get the details we're interested in:
    details_work <- list(
      PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
      DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
      TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
      LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
      AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
      FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
      AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
      LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
      PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
      TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
      COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
      JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
    )
    
    # Convert the list of details into a one-row data frame:
    details_df_work <- bind_rows(details_work)
    
    return(details_df_work)
  })
})

# Check the results:
article_details_work %>%
  as_tibble() %>%
  print()

# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
             'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
             'COIS- ' = '', 'JT  - ' = '')

results_work <- article_details_work %>%
  mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
  # Now separate the date year and month/day info:
  separate(DP, c("Year", "Month"), sep = "\\ ") %>%
  mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
  rename(# Let's also provide more descriptive names for each of the columns
         "Title" = "TI", 
         "DOI" = "LID", 
         "Abstract" = "AB", 
         "Full_Author_Name" = "FAU", 
         "Author_Address" = "AD", 
         "Language" = "LA", 
         "Publication_Type" = "PT", 
         "Journal_name_short" = "TA",
         "Conflict_of_Interest_Statement" = "COIS",
         "Journal_Title" = "JT") %>%
  print()


# Now save the initial results before proceeding so we can't lose them!
write.csv(results_work, "Data extraction/work_data.csv", row.names=FALSE)
```

#### Extract full title using rvest

Again, we're going to use the `rvest` package to scrape the titles returned from the manual search in PubMed, then join the two datasets using the PMID, retaining the title from this new dataset.

```{{r}}
# Define the URL
url_work_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%22work+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22addiction+to+work%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22workaholism%22%5BTitle%2FAbstract%5D%29&size=200"

# Read and parse the webpage
webpage_work_rvest <- read_html(url_work_rvest)

# Get the total number of search results
results_count_work <- webpage_work_rvest %>%
  html_node(".results-amount .value") %>%
  html_text() %>%
  str_replace(",", "") %>%
  as.numeric()

# Calculate the number of pages
results_per_page_work <- 50
total_pages_work <- ceiling(results_count_work[1] / results_per_page_work)
# Print results_count_work and total_pages_work
print(results_count_work)
print(total_pages_work)


# Initialize an empty data frame
results_work_rvest <- data.frame()

# Loop through each page and scrape the data
for (page in 0:(total_pages_work - 1)) {
  # Update the page parameter in the URL for the current page
  current_url <- paste0(url_work_rvest, "&page=", page + 1)
  
  # Read and parse the current webpage
  current_page <- read_html(current_url)
  
  # Extract title
  Title <- tryCatch({
    current_page %>%
      html_nodes(".docsum-title") %>%
      html_text(trim = TRUE)
  }, error = function(e) {
    print(paste("Error extracting title info on page", page + 1))
    rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
  })
  
  # Extract the PMID
  pmid <- current_page %>%
    html_nodes(".docsum-pmid") %>%
    html_text(trim = TRUE)
  
  # Create a data frame with the extracted data
  current_results_work <- data.frame(Title = Title,
                                       PMID = pmid)
  
  # Combine the current results with the previous results
  results_work_rvest <- rbind(results_work_rvest, current_results_work)
  
  # Introduce a delay to avoid overloading the server
  sleep_time <- runif(1, min = 5, max = 15)
  Sys.sleep(sleep_time)
}

# Print the extracted data
print(as_tibble(results_work_rvest))
```

#### Refine dataset

Now we need to identify and remove any duplicates, irrelevant studies, or entries with any other issues (e.g., those that turn out to be books)

```{r}
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_work<- read.csv("Data extraction/work_data.csv") %>% 
  as_tibble()

# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_work_rvest<- read.csv("Data extraction/work_titles.csv") %>% 
  as_tibble()

# Merge our two datasets:
results_work2 <- results_work %>%
  as_tibble() %>%
  rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
  full_join(results_work_rvest, by = "PMID") %>%
  select(-Title_trunc) %>% # Bye truncated title
  relocate(Title) %>% # Place full title first for ease
  print()


# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_work2 %>%
  distinct(Title) %>%
  count() %>% 
  print() # None

# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_work2 %>%
  distinct(PMID) %>%
  count() %>% 
  print() # None

# Have a look at duplicates where the title, year, and authors are all the same:
results_work2 %>% 
  group_by(Title, 
           Journal_name_short, 
           Full_Author_Name) %>% 
  filter(n()>1) # None

# Have a look at duplicates were just the title, journal and year are the same:
results_work2 %>% 
  group_by(Title, 
           Journal_name_short, 
           Year) %>% 
  filter(n()>1)# None

# Upon manual inspection of the dataset, there appears to be one study with the title "Overearning"  which doesn't seem to meet my criteria for inclusion.  Therefore, let's remove this here:


# Visually inspect for duplicates/Check results:
# View(results_work2)

results_work2_relevent <- results_work2 %>% 
   filter(Title != "Overearning.") # There is a "." at the end of the total in the extracted data for some reason
# Confirming that I have checked this reduces the number of studies by 1.

#  Okay, now we are confident that we have a clean dataset, let's explore!

# Summary of the Year  variable:
summary(results_work2_relevent$Year)

# Number of publications per year:
work_hist<- hist(results_work2_relevent$Year,
                     xlim = c(1960,2023),
                     breaks = 40,
                     main = "Papers published per year on Work Addiction (PubMed)",
                     xlab = "Year")

# Number of separate journals:
results_work2_relevent %>% 
  distinct(Journal_name_short) %>% 
  arrange(Journal_name_short) %>% 
  print(n = 20)

# Most popular journals:
results_work2_relevent %>% 
  group_by(Journal_name_short) %>% 
  summarise(
    n = n()
  ) %>% 
  arrange(desc(n)) %>%
  print(n = 20)

# Now I need to add a label to all of these studies to signify that they Were returned from the work search so that we can distinguish them from other studies results_work2 we later joined the datasets together:

Label_work <- rep("work", times = count(results_work2_relevent))

results_work_final <- results_work2_relevent %>% 
  bind_cols(Label_work) %>%
  rename(Label = 14) %>%
  print()

# View(results_work_final)
# Now save the cleaned results
write.csv(results_work_final, "Data extraction/work_data_cleaned.csv", row.names=FALSE)
```

## Exercise addiction

#### Extract studies and perform a basic clean of dataset

**Notes pre-search:** No MeSH terms for this, so I looked up the most recent systematic review on the topic that I could find (https://doi.org/10.1007/s11469-021-00568-1) and then examine their search strategy. They use the following terms: exercise addiction, exercise dependence, compulsory exercise, obligatory exercis

**Actual search used:** (((("Exercise addiction"\[Title/Abstract\]) OR ("exercise dependence"\[Title/Abstract\])) OR ("compulsory exercise"\[Title/Abstract\])) OR ("obligatory exercise"\[Title/Abstract\])) OR ("Addiction to exercise"\[Title/Abstract\])

**Manual search results:** [link](https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%22Exercise+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22exercise+dependence%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22compulsory+exercise%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22obligatory+exercise%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22Addiction+to+exercise%22%5BTitle%2FAbstract%5D%29&size=200)

**Notes post-search:**I manually inspected all of the results returned on PubMed. The search term " compulsory exercise" is slightly problematic for this search as it sometimes used in the context of exercise rehabilitation/animal studies. The following studies were found to be not relevant:

-   "\[Doping in sports\]."

-   "Psychoeducation: a basic psychotherapeutic intervention for patients with schizophrenia and their families."

-   "Metabolic response to fasting in experimental intrauterine growth retardation induced by surgical and nonsurgical maternal stress."

-   "\[Arguments in favor of Integrated Health Care as regular health care provision in cardiology\]."

-   "\[What can a chair on alternatives to animal experimentation effectuate?\]."

-   "Mandatory Physical Education Classes of Two Hours per Week Can Be Comparable to Losing More than Five Kilograms for Chinese College Students."

-   "Degeneration of dystrophic or injured skeletal muscles induces high expression of Galectin-1."

-   "A Pilot Study of a 12-Week Leg Exercise and a 6- and 12-Month Follow-Up in Community-Dwelling Diabetic Elders: Effect on Dynamic Standing Balance."

-   "Opposite effects of catalase and MnSOD ectopic expression on stress induced defects and mortality in the desmin deficient cardiomyopathy model."

-   "Beef extract supplementation increases leg muscle mass and modifies skeletal muscle fiber types in rats."

-   "Does exercise deprivation increase the tendency towards morphine dependence in rats?."

-   "A Polymeric Bilayer Multi-Legged Soft Millirobot with Dual Actuation and Humidity Sensing."

    # EXCLUDE THE ABOVE STUDIES FROM THE FINAL DATASET!

```{{r}}
#  Clean environment:
rm(list = ls())

# Define the search term:
search_term_exercise <- '(((("Exercise addiction"[Title/Abstract]) OR ("exercise dependence"[Title/Abstract])) OR ("compulsory exercise"[Title/Abstract])) OR ("obligatory exercise"[Title/Abstract])) OR ("Addiction to exercise"[Title/Abstract])'

# Use entrez_search to get the IDs of the articles:
search_results_exercise <- entrez_search(db="pubmed", term=search_term_exercise, retmax=20000)
id_list_exercise <- search_results_exercise$ids

# Split id list into chunks of 500:
chunks <- split(id_list_exercise, ceiling(seq_along(id_list_exercise)/100))

# Fetch details for each chunk of articles:
article_details_exercise <- map_dfr(chunks, function(ids) {
  # Fetch details of the articles:
  articles <- entrez_fetch(db="pubmed", id=ids, rettype="medline", retmode="text")
  
  # Split the articles into individual articles:
  articles <- strsplit(articles, "\n\n")[[1]]
  
  # Process each article:
  map_dfr(articles, function(article) {
    # Split the article into lines:
    lines <- strsplit(article, "\n")[[1]]
    
    # Get the details we're interested in:
    details_exercise <- list(
      PMID = if (any(grepl("^PMID", lines))) lines[grepl("^PMID", lines)] else NA,
      DP = if (any(grepl("^DP", lines))) lines[grepl("^DP", lines)] else NA,
      TI = if (any(grepl("^TI", lines))) lines[grepl("^TI", lines)] else NA,
      LID = if (any(grepl("^LID", lines))) paste(lines[grepl("^LID", lines)], collapse="; ") else NA,
      AB = if (any(grepl("^AB", lines))) paste(lines[grepl("^AB", lines)], collapse=" ") else NA,
      FAU = if (any(grepl("^FAU", lines))) paste(lines[grepl("^FAU", lines)], collapse="; ") else NA,
      AD = if (any(grepl("^AD", lines))) paste(lines[grepl("^AD", lines)], collapse="; ") else NA,
      LA = if (any(grepl("^LA", lines))) paste(lines[grepl("^LA", lines)], collapse="; ") else NA,
      PT = if (any(grepl("^PT", lines))) paste(lines[grepl("^PT", lines)], collapse="; ") else NA,
      TA = if (any(grepl("^TA", lines))) lines[grepl("^TA", lines)] else NA,
      COIS = if (any(grepl("^COIS", lines))) paste(lines[grepl("^COIS", lines)], collapse=" ") else NA,
      JT = if (any(grepl("^JT", lines))) lines[grepl("^JT", lines)] else NA
    )
    
    # Convert the list of details into a one-row data frame:
    details_df_exercise <- bind_rows(details_exercise)
    
    return(details_df_exercise)
  })
})

# Check the results:
article_details_exercise %>%
  as_tibble() %>%
  print()

# Let's remove the identifiers at the beginning of each data point:
rep_str <- c('PMID- ' = '', 'DP  - ' = '', 'TI  - ' = '', 'LID - ' = '', 'AB  - ' = '',
             'FAU - ' = '', 'AD  - ' = '', 'LA  - ' = '', 'PT  - ' = '', 'TA  - ' = '',
             'COIS- ' = '', 'JT  - ' = '')

results_exercise <- article_details_exercise %>%
  mutate(across(everything(), ~str_replace_all(., rep_str))) %>% # Remove identifiers
  # Now separate the date year and month/day info:
  separate(DP, c("Year", "Month"), sep = "\\ ") %>%
  mutate(Year = as.numeric(Year)) %>% # Convert Year to numeric
  rename(# Let's also provide more descriptive names for each of the columns
         "Title" = "TI", 
         "DOI" = "LID", 
         "Abstract" = "AB", 
         "Full_Author_Name" = "FAU", 
         "Author_Address" = "AD", 
         "Language" = "LA", 
         "Publication_Type" = "PT", 
         "Journal_name_short" = "TA",
         "Conflict_of_Interest_Statement" = "COIS",
         "Journal_Title" = "JT") %>%
  print()

# Now save the initial results before proceeding so we can't lose them!
write.csv(results_exercise, "Data extraction/exercise_data.csv", row.names=FALSE)

```

#### Extract full title using rvest

Again, we're going to use the `rvest` package to scrape the titles returned from the manual search in PubMed, then join the two datasets using the PMID, retaining the title from this new dataset.

```{{r}}
# Add search URL:
url_exercise_rvest <- "https://pubmed.ncbi.nlm.nih.gov/?term=%28%28%28%28%22Exercise+addiction%22%5BTitle%2FAbstract%5D%29+OR+%28%22exercise+dependence%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22compulsory+exercise%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22obligatory+exercise%22%5BTitle%2FAbstract%5D%29%29+OR+%28%22Addiction+to+exercise%22%5BTitle%2FAbstract%5D%29&size=200" # 386 results returned on 10/07/2023

# Read and parse the webpage
webpage_exercise_rvest <- read_html(url_exercise_rvest)

# Get the total number of search results
results_count_exercise <- webpage_exercise_rvest %>%
  html_node(".results-amount .value") %>%
  html_text() %>%
  str_replace(",", "") %>%
  as.numeric()

# Calculate the number of pages
results_per_page_exercise <- 50
total_pages_exercise <- ceiling(results_count_exercise[1] / results_per_page_exercise)
# Print results_count_exercise and total_pages_exercise
print(results_count_exercise)
print(total_pages_exercise)


# Initialize an empty data frame
results_exercise_rvest <- data.frame()

# Loop through each page and scrape the data
for (page in 0:(total_pages_exercise - 1)) {
  # Update the page parameter in the URL for the current page
  current_url <- paste0(url_exercise_rvest, "&page=", page + 1)
  
  # Read and parse the current webpage
  current_page <- read_html(current_url)
  
  # Extract title
  Title <- tryCatch({
    current_page %>%
      html_nodes(".docsum-title") %>%
      html_text(trim = TRUE)
  }, error = function(e) {
    print(paste("Error extracting title info on page", page + 1))
    rep(NA, length(Title))  # Return a vector of NAs with the same length as Title
  })
  
  # Extract the PMID
  pmid <- current_page %>%
    html_nodes(".docsum-pmid") %>%
    html_text(trim = TRUE)
  
  # Create a data frame with the extracted data
  current_results_exercise <- data.frame(Title = Title,
                                       PMID = pmid)
  
  # Combine the current results with the previous results
  results_exercise_rvest <- rbind(results_exercise_rvest, current_results_exercise)
  
  # Introduce a delay to avoid overloading the server
  sleep_time <- runif(1, min = 5, max = 15)
  Sys.sleep(sleep_time)
}

# Print the extracted data
print(as_tibble(results_exercise_rvest))


# Now save the titles before proceeding so we can't lose them!
write.csv(results_exercise_rvest, "Data extraction/exercise_titles.csv", row.names=FALSE)
```

#### Refine dataset

Now we need to identify and remove any duplicates, irrelevant studies, or entries with any other issues (e.g., those that turn out to be books)

```{r}
# Load in first dataset in case environment clean on knit (above sections not executed for render):
results_exercise<- read.csv("Data extraction/exercise_data.csv") %>% 
  as_tibble()

# Load in second dataset in case environment clean on knit (above sections not executed for render):
results_exercise_rvest<- read.csv("Data extraction/exercise_titles.csv") %>% 
  as_tibble()

# Merge our two datasets
results_exercise2 <- results_exercise %>%
  as_tibble() %>%
  rename("Title_trunc" = "Title") %>% # Designate the truncated title from our primary dataset so we can remove it once we get the full title
  full_join(results_exercise_rvest, by = "PMID") %>%
  select(-Title_trunc) %>% # Bye truncated title
  relocate(Title) %>% # Place full title first for ease
  print()

# Check results:
# View(results_exercise2)

# Total number with basic duplication removal based on title:
Simple_duplicate_removal_n <- results_exercise2 %>%
  distinct(Title) %>%
  count() %>% 
  print() # n = 2 dupes (jUly 2023)

# Show duplicate:
results_exercise2 %>% 
  group_by(Title) %>% 
  filter(n()>1) %>% 
  # View()
  print() # These all have different PMIDs, DOIs, authors, and years


# Have a look at duplicates where the title, year, and authors are all the same:
results_exercise2 %>% 
  group_by(Title, 
           Journal_name_short, 
           Full_Author_Name) %>% 
  filter(n()>1)
  # print() #None

# Have a look at duplicates were just the title, journal and year are the same:
results_exercise2 %>% 
  group_by(Title, 
           Journal_name_short, 
           Year) %>% 
  filter(n()>1) %>% 
  print() # None

#  Okay, now we're confident we have a clean dataset, let's explore!

# Summary of the Year  variable:
summary(results_exercise2$Year)

# Number of publications per year:
exercise_hist<- hist(results_exercise2$Year,
                     xlim = c(1960,2023),
                     breaks = 40,
                     main = "Papers published per year on exercise Disorder (PubMed)",
                     xlab = "Year")

# Number of separate journals:
results_exercise2 %>% 
  distinct(Journal_name_short) %>% 
  arrange(Journal_name_short) %>% 
  print(n = 20)

# Most popular journals:
results_exercise2 %>% 
  group_by(Journal_name_short) %>% 
  summarise(
    n = n()
  ) %>% 
  arrange(desc(n)) %>%
  print(n = 20)

# View(results_exercise2) # Visually inspect results

# Now I need to add a label to all of these studies to signify that they Were returned from the exercise search so that we can distinguish them from other studies when we later joined the datasets together:
Label <- rep("exercise", times = count(results_exercise2))

results_exercise_final <- results_exercise2 %>% bind_cols(Label) %>%
  rename(Label = 14) %>%
  print()

# View(results_exercise_final)
# Now save the cleaned results
write.csv(results_exercise_final, "Data extraction/exercise_data_cleaned.csv", row.names=FALSE)
```
